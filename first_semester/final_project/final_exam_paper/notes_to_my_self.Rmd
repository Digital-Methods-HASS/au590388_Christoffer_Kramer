---
title: "Noter til mig selv"
author: "Christoffer Kramer"
date: "19/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
library(tidyverse)
library(tidytext)
library(dplyr)
library(ggplot2)
library(tidyr)
library(ggwordcloud)
library(wordcloud)
library(reshape2)
library(qdap)
```

## Noter til mig selv

Hav nogle scripts, opdel, sæt tal foran, for at vise, hvad der skal køres først
Tilføj eventuelt et scipt for plots.
Sæt kun funktioner i et seperat script, hvis du skal bruge det i mange scripts.
Gør det klart, at der måske er fejl i antagelsen om,at den næste tomme celle er den tidligere taler (transkriptionsfejl).
Kan måske testes, er der nogle "others", der siger meget?
```{r}
# Tokenize, remove digits, remove uh and uhh ------------------------------
my_stop_words <- tibble(word = c("uh", "uhh"))

debate_words <- all_debates %>% 
  unnest_tokens(word, text) %>% 
  anti_join(my_stop_words) %>% 
  filter(is.na(as.numeric(word))) %>%
  count(year, party, word, sort = TRUE)

all_years <- debate_stop_words_removed

# Stop word filtering -------------------------------------------
debates_stop_words_removed <- debate_words %>% 
  mutate(word = gsub("\u2019", "'", word)) %>%  
  anti_join(stop_words) %>% 
  group_by(year)
view(debates_stop_words_removed)
view(debate_stop_words_removed)

#Get a list of all years for plotting and loops
all_years <- debate_stop_words_removed %>% 
  select(year) %>% 
  unique() %>% 
  arrange(desc(year))



```


```{r}

for (i in all_years) {
print(i)
 plot <- debates_stop_words_removed %>%
    filter(party != "not_a_candidate") %>%
    filter(party != "I" ) %>%
    filter(year == i) %>%
    acast(word ~ party, fun.aggregate = sum, value.var = "n", fill = 0) %>%
    comparison.cloud(colors = c("gray20", "gray80"),
                     max.words = 20)
 print(plot)
 #(paste(i,"word_cloud.pdf", sep = "_"), width = 65, height = 35, units = "cm")
}
```

```{r}
affin_republicans %>%  
  count(year, value) %>% 
  ggplot(aes(x = value, y = n)) + 
  geom_col() +
  facet_wrap(~year, ncol = 3, scales = "free") +
  labs(title = "Republicans overall sentiment",
       x = "sentiment",
       y = "count")

affin_democrats %>%  
  count(value) %>% 
  group_by(year) %>% 
  ggplot(aes(x = value, y = n)) + 
  geom_col() +
  facet_wrap(~year, ncol = 3, scales = "free") +
    labs(title = "Democrats overall sentiment",
       x = "sentiment",
       y = "count")


```



# EKSTRA SHIT

```{r}
debate_words_party %>% 
    filter(party == "D") %>% 
    top_n(20) %>% 
    ggplot(aes(label = word, size = n)) +
    geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
    scale_size_area(max_size = 12) +
    scale_color_gradientn(colors = c("darkgreen","blue","red")) +
    theme_minimal()

ggsave("word_cloud_dems.pdf", width = 65, height = 35, units = "cm")
```



### Republicans
```{r, echo=FALSE}
debates_stop_words_removed %>% 
  filter(party != "R") %>%
  group_by(year, party) %>% 
  top_n(20) %>% 
  ungroup %>% 
  ggplot(aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
  #scale_size_area(max_size = 12) +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
  theme_minimal() +
  facet_wrap(~year, ncol = 3, scales = "free")

ggsave("word_cloud_rep.pdf", width = 65, height = 35, units = "cm")
```



```{r}
debate_words_party %>% 
    filter(party == "R") %>% 
    top_n(20) %>% 
    ggplot(aes(label = word, size = n)) +
    geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
    scale_size_area(max_size = 12) +
    scale_color_gradientn(colors = c("darkgreen","blue","red")) +
    theme_minimal()
```


### Both Parties

```{r}
dems <- debates_stop_words_removed %>% 
  filter(party == "D")


reps <- debates_stop_words_removed %>% 
  filter(party == "R")


shared_words <- semi_join(reps, dems, by = c("word" = "word"))

shared_words %>% 
    top_n(20) %>% 
    ggplot(aes(label = word, size = n)) +
    geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
    scale_size_area(max_size = 12) +
    scale_color_gradientn(colors = c("darkgreen","blue","red")) +
    theme_minimal()
```

```{r}

shared_words_all_years <- semi_join(reps, dems, by = c("word" = "word"))

debates_stop_words_removed %>% 
    group_by(year) %>% 
    top_n(20) %>% 
    ungroup %>% 
    ggplot(aes(label = word, size = n)) +
    geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
    scale_size_area(max_size = 12) +
    scale_color_gradientn(colors = c("darkgreen","blue","red")) +
    theme_minimal() +
    facet_wrap(~year, ncol = 2, scales = "free")
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

## TF_IDF all debates
```{r}
total_words_all <- all_debates %>% 
  unnest_tokens(word, text) %>% 
  count(year, word, sort = TRUE) %>% 
  group_by(year) %>% 
  summarise(total = sum(n))

words_count <- all_debates %>% 
  unnest_tokens(word, text) %>% 
  count(year, word, sort = TRUE) %>% 
  left_join(total_words_all, by = "year")

words_count


words_tf_idf <- words_count %>% 
  bind_tf_idf(word, year, n) 

words_tf_idf
```

### Plot all_debates
```{r}
words_tf_idf %>% 
  arrange(desc(tf_idf)) %>% 
  group_by(year) %>% 
  top_n(20) %>% 
    ungroup %>% 
    ggplot(aes(label = word, size = tf_idf)) +
    geom_text_wordcloud_area(aes(color = tf_idf), shape = "diamond") +
    scale_size_area(max_size = 12) +
    scale_color_gradientn(colors = c("darkgreen","blue","red")) +
    theme_minimal() +
    facet_wrap(~year, ncol = 3, scales = "free")

ggsave("word_cloud_tf_idf.pdf", width = 65, height = 35, units = "cm")
```

### TF R and D

```{r}
total_words_all <- all_debates %>% 
  filter(party != "I") %>% 
  filter(party != "not_a_candidate") %>% 
  unnest_tokens(word, text) %>% 
  count(party, word, sort = TRUE) %>% 
  group_by(party) %>% 
  summarise(total = sum(n))

words_count <- all_debates %>% 
  filter(party != "I") %>% 
  filter(party != "not_a_candidate") %>%  
  unnest_tokens(word, text) %>% 
  count(party, word, sort = TRUE) %>% 
  left_join(total_words_all, by = "party")



words_tf_idf <- words_count %>% 
  bind_tf_idf(word, party, n) 

words_tf_idf %>% 
  arrange(desc(tf_idf)) %>% 
  group_by(party) %>% 
  top_n(50) %>% 
    ungroup %>% 
    ggplot(aes(label = word, size = tf_idf)) +
    geom_text_wordcloud_area(aes(color = tf_idf), shape = "diamond") +
    scale_size_area(max_size = 12) +
    scale_color_gradientn(colors = c("darkgreen","blue","red")) +
    theme_minimal() +
    facet_wrap(~party, ncol = 2, scales = "free")




```

### TF All
```{r}
total_words_all <- all_debates %>% 
  unnest_tokens(word, text) %>% 
  filter(!grepl("[[:digit:]]", word)) %>% #Remove digits (digits seperated with commas included e.g. 1,000)
  count(party, word, sort = TRUE) %>% 
  group_by(party) %>% 
  summarise(total = sum(n))

words_count <- all_debates %>% 
  unnest_tokens(word, text) %>% 
  filter(!grepl("[[:digit:]]", word)) %>% #Remove digits (digits seperated with commas included e.g. 1,000)
  count(party, word, sort = TRUE) %>% 
  left_join(total_words_all, by = "party")



words_tf_idf <- words_count %>% 
  bind_tf_idf(word, party, n) 

words_tf_idf %>% 
  arrange(desc(tf_idf)) %>% 
  group_by(party) %>% 
  top_n(50) %>% 
    ungroup %>% 
    ggplot(aes(label = word, size = tf_idf)) +
    geom_text_wordcloud_area(aes(color = tf_idf), shape = "diamond") +
    scale_size_area(max_size = 12) +
    scale_color_gradientn(colors = c("darkgreen","blue","red")) +
    theme_minimal() +
    facet_wrap(~party, ncol = 2, scales = "free")

```

## Sentiment analysis
### Afinn

```{r}
dems <- all_debates %>% 
  filter(party == "D") %>% 
  unnest_tokens(word, text) %>% 
  filter(!grepl("[[:digit:]]", word)) %>%
  count(party, word, sort = TRUE)
  
reps <- all_debates %>% 
 filter(!grepl("[[:digit:]]", word)) %>%
  unnest_tokens(word, text) %>% 
  filter(is.na(as.numeric(word))) %>%
  count(party, word, sort = TRUE)

# Affin Democrats and Republicans -----------------------------------------
affin_R <- reps %>% 
  inner_join(get_sentiments("afinn"))

affin_D <- dems %>% 
  inner_join(get_sentiments("afinn"))
```


```{r}
# Plot Afinn --------------------------------------------------------------
affin_R_plot <- affin_R %>% 
  count(value) %>% 
  ggplot(aes(x = value, y = n)) + 
  geom_col()
affin_R_plot
```


```{r}
affin_D_plot <- affin_D %>% 
  count(value) %>% 
  ggplot(aes(x = value, y = n)) + 
  geom_col()
affin_D_plot
```

###Afinn all years Democrats
```{r}
dems <- all_debates %>% 
  filter(party == "D") %>% 
  unnest_tokens(word, text) %>% 
  filter(is.na(as.numeric(word))) %>%
  count(party, year, word, sort = TRUE) %>% 
  inner_join(get_sentiments("afinn"))
  

affin_D_plot <- dems %>% 
  group_by(year) %>% 
  count(value) %>% 
  ggplot(aes(x = value, y = n)) + 
  geom_col() +
  facet_wrap(~year, ncol = 3, scales = "free")
affin_D_plot




```

###Afinn all years republicans
```{r}
reps <- all_debates %>% 
  filter(party == "R") %>% 
  unnest_tokens(word, text) %>% 
  filter(is.na(as.numeric(word))) %>%
  count(party, year, word, sort = TRUE) %>% 
  inner_join(get_sentiments("afinn"))

affin_R_plot <- dems %>% 
  group_by(year) %>% 
  count(value) %>% 
  ggplot(aes(x = value, y = n)) + 
  geom_col() +
  facet_wrap(~year, ncol = 3, scales = "free")
affin_D_plot
```

### NRC
```{r}
# NRC ---------------------------------------------------------------------
nrc_R <- dems %>% 
  inner_join(get_sentiments("nrc"))

nrc_D <- reps %>% 
  inner_join(get_sentiments("nrc"))

# Plot NRC ----------------------------------------------------------------
ggplot(data = nrc_R, aes(x = sentiment, y = n)) +
  geom_col()

ggplot(data = nrc_D, aes(x = sentiment, y = n)) +
  geom_col()
```


### NRC all years dems
```{r}
dems <- all_debates %>% 
  filter(party == "D") %>% 
  unnest_tokens(word, text) %>% 
  filter(is.na(as.numeric(word))) %>%
  count(party, year, word, sort = TRUE) %>% 
  inner_join(get_sentiments("nrc"))

dems %>% 
  group_by(year) %>% 
  count(sentiment) %>% 
  ggplot(aes(x = sentiment, y = n)) + 
  geom_col() +
  facet_wrap(~year, ncol = 3, scales = "free")

```


### NRC all years reps
```{r}
reps <- all_debates %>% 
  filter(party == "R") %>% 
  unnest_tokens(word, text) %>% 
  filter(is.na(as.numeric(word))) %>%
  count(party, year, word, sort = TRUE) %>% 
  inner_join(get_sentiments("nrc"))

reps %>% 
  group_by(year) %>% 
  count(sentiment) %>% 
  ggplot(aes(x = sentiment, y = n)) + 
  geom_col() +
  facet_wrap(~year, ncol = 3, scales = "free")

```



###BING

```{r}
# bing --------------------------------------------------------------------
bing_R <- reps %>% 
  inner_join(get_sentiments("bing"))

bing_D <- dems%>% 
  inner_join(get_sentiments("bing"))

# Plot bing ---------------------------------------------------------------)
ggplot(data = bing_R, aes(x = sentiment, y = n)) +
  geom_col()

ggplot(data = bing_D, aes(x = sentiment, y = n)) +
  geom_col()


```
### BING all years dems
```{r}
dems <- all_debates %>% 
  filter(party == "D") %>% 
  unnest_tokens(word, text) %>% 
  filter(is.na(as.numeric(word))) %>%
  count(party, year, word, sort = TRUE) %>% 
  inner_join(get_sentiments("bing"))

dems %>%
  group_by(year) %>% 
  count(sentiment) %>% 
  ggplot(aes(x = sentiment, y = n)) + 
  geom_col() +
  facet_wrap(~year, ncol = 3, scales = "free")

```


### BING all years reps
```{r}
reps <- all_debates %>% 
  filter(party == "R") %>% 
  unnest_tokens(word, text) %>% 
  filter(is.na(as.numeric(word))) %>%
  count(party, year, word, sort = TRUE) %>% 
  inner_join(get_sentiments("bing"))

reps %>%
  group_by(year) %>% 
  count(sentiment) %>% 
  ggplot(aes(x = sentiment, y = n)) + 
  geom_col() +
  facet_wrap(~year, ncol = 3, scales = "free")
```



## NETWORKS



## Ekstra shit
```{r}
# debates_only_R_and_D <- debates_stop_words_removed %>%
#     filter(party != "not_a_candidate") %>%
#     filter(party != "I" ) 
# 
# # print(all_years)
# # for (i in seq_along(all_years)) {
# #   print(i)
# # # data <-  debates_only_R_and_D %>%
# # #     filter(year == i) %>%
# # #    acast(word ~ party, fun.aggregate = sum, value.var = "n", fill = 0)
# # # print(data)
# # }
# 
# test <- 
#   debates_only_R_and_D %>% 
#   split(.$year)
# 
# # test %>% 
# #   walk(print)
# # 
# # test
# # walk(all_years, debates_only_R_and_D %>% 
# #     acast(word ~ party, fun.aggregate = sum, value.var = "n", fill = 0) %>% 
# #     comparison.cloud(colors = c("blue", "red"),
# #                      max.words = 20))
# # 
# 
# # map()
# test1 <- all_debates$year %>% unique()
# test1 <- c(test1)
# test1 <- all_debates[["year"]]
# test1 <- test1 %>%  unique()
# test1 <- c(test1)
# test1
# length(test1)
# 
# for (i in test) {
# label_year <- i$year %>% unique()
#  i %>%
#  acast(word ~ party, value.var = "n", fill = 0) %>%
#  comparison.cloud(colors = c("blue", "red"),
#  max.words = 20,
#  scale = c(2,0.25),
#  title.size = 1,
#  random.order = TRUE,
#  match.colors = TRUE)
# 
# #ggsave(paste(j,"word_cloud.png", sep = "_"), width = 65, height = 35, units = "cm")
# }
# 
#  testing <- i %>%
#     acast(word ~ party, fun.aggregate = sum, value.var = "n", fill = 0)
#   view(testing)
#   
#   
#   for(i in test1){
#     
#     debates_stop_words_removed %>% 
#       filter(part == "D")
#     
#    acast(word ~ party, fun.aggregate = sum, value.var = "n", fill = 0) %>%
#    comparison.cloud(colors = c("blue", "red"),
#    max.words = 50,
#    scale = "c(2,0.25)",
#    title.size = 1,
#    random.order = FALSE,
#    match.colors = TRUE)
#     }
# 
#  ggsave("word_cloud_test.pdf", width = 65, height = 35, units = "cm")
# all_years <- all_debates %>% 
#   filter(party == "R")
# 
# test <- all_debates %>% 
#   unnest_tokens(word, text) %>% 
#   anti_join(my_stop_words) %>% 
#   filter(is.na(as.numeric(word))) %>%
#   count(word, sort = TRUE)
# 
# 
# test <- test %>% 
#   mutate(word = gsub("\u2019", "'", word)) %>%  
#   anti_join(stop_words)
# 
# 
# # test <- debates_only_R_and_D %>% acast(word ~ n, value.var = "n", fill = 0) 
# # view(test)
# wordcloud2(test)
# view(data)

# 
# debate_words_party <- all_debates %>% 
#   unnest_tokens(word, text) %>% 
#   anti_join(my_stop_words) %>% 
#   filter(is.na(as.numeric(word))) %>%
#   count(party, word, sort = TRUE) %>% 
#   mutate(word = gsub("\u2019", "'", word)) %>%  
#   anti_join(stop_words)
# 
# # Stop word filtering -------------------------------------------
# debates_stop_words_removed <- debate_words %>% 
#   mutate(word = gsub("\u2019", "'", word)) %>%  
#   anti_join(stop_words)
# 
# 
# test <- debates_stop_words_removed %>% 
#   group_by(word) %>% 
#   summarise(word_n = n())
#   view(debate_words_party)
#Get a list of all years for plotting and loops
# all_years <- map_dbl((debates_stop_words_removed %>% 
#   select(year) %>% 
#   unique() %>% 
#   arrange(desc(year))))
# 
# map_dbl()
# all_years


```

```{r}
splitted_by_year_position_included <- all_debates %>% 
  unnest_tokens(word, text) %>% 
  filter(!grepl("[[:digit:]]", word)) %>% #Remove digits (digits seperated with commas included e.g. 1,000)
  count(year, position, party, word, sort = TRUE) %>% 
  mutate(word = gsub("\u2019", "'", word)) %>%  #Make read it as unix
  anti_join(stop_words) %>% 
  anti_join(my_stop_words) %>% 
  split(.$year)
  
  for (df in splitted_by_year_position_included) {

plot_presidents_democrats <-  df %>%
  filter(party == "D") %>%
  filter(position == "president") %>% 
    plot_wordclouds() +
  labs(title = "Democrats",
       subtitle = unique(df$year))

plot_presidents_republicans <-  df %>%
  filter(party == "R") %>%
  filter(position == "president") %>% 
    plot_wordclouds() +
  labs(title = "Republicans",
       subtitle = unique(df$year))

  print(plot_presidents_democrats)
  print(plot_presidents_republicans)

  
}


```



```{r pressure, echo=FALSE}
splitted_by_year <- stop_words_removed %>% 
  split(.$year)

for (df in splitted_by_year) {

plot_democrats <-  df %>%
  filter(party == "D") %>%
    plot_wordclouds() +
  labs(title = "Democrats",
       subtitle = unique(df$year))

plot_republicans <-  df %>%
  filter(party == "R") %>%
    plot_wordclouds() +
  labs(title = "Republicans",
       subtitle = unique(df$year))

  print(plot_democrats)
  print(plot_republicans)

  
}
```

```{r}
word_tf_idf_splitted_by_year <- word_tf_idf %>%
  split(.$year)
for (df in word_tf_idf_splitted_by_year) {
 plot <- plot_wordclouds(data_set = df)
print(plot)
 }  

```

```{r}


democrats <- debate_words %>% 
  filter(party == "D") %>% 
  inner_join(get_sentiments("afinn")) %>% 
  inner_join(get_sentiments("bing"))


republicans <- debate_words %>% 
  filter(!grepl("[[:digit:]]", word)) %>%
  count(word, sort = TRUE) %>% 
  inner_join(get_sentiments("afinn")) %>% 
  inner_join(get_sentiments("bing"))


```



```{r}

sentiment_debate_words %>% 
  filter(party == "D") %>%  
  filter(year >= 1992) %>% 
  group_by(year) %>% 
  count(value) %>% 
  ggplot(aes(x = value, y = n)) + 
  geom_col() +
  facet_wrap(~year, ncol = 3, scales = "fixed")+
  labs(title = "Democrats sentiment each year - AFINN",
       x = "sentiment",
       y = "count")

sentiment_debate_words %>% 
  filter(party == "R") %>% 
  filter(year >= 1992) %>% 
  group_by(year) %>% 
  count(value) %>%
  ggplot(aes(x = value, y = n)) + 
  geom_col() +
  facet_wrap(~year, ncol = 3, scales = "fixed")+
  labs(title = "Republicans sentiment each year - AFINN",
       x = "sentiment",
       y = "count")


sentiment_debate_words %>% 
  filter(year >= 1992) %>% 
  group_by(year) %>% 
  count(value) %>% 
  ggplot(aes(x = value, y = n)) + 
  geom_col() +
  facet_wrap(~year, ncol = 3, scales = "fixed") +
  labs(title = "Overall sentiment each year - AFINN",
       x = "sentiment",
       y = "count")


```

# Cleaning

Then I make a list of correct dates, which have the same index
```{r cleaning dates}
#Make a list of correct dates
right_dates <- c("october-7-2020", "september-26-2008", "october-2-2008", "september-26-2008")
```

```{r find dates}
#Remove translations page from list of dates and the dataset
debate_dates <- debate_dates[!(debate_dates == "/voter-education/debate-transcripts/2000-debate-transcripts-translations/")]
all_debates <- all_debates[!(all_debates$date == "/voter-education/debate-transcripts/2000-debate-transcripts-translations/"),]
```

```{r cleaning dates loop}
i <- 0 #Index
#Loop through every wrong date and replace it with a name from the vector "right_dates" in all_debates and the vector debate_dates
for (date in wrong_dates) {
  i <- i+1  
  all_debates <- clean_dates(all_debates, old_pattern = date, new_replacement = right_dates[i])
  debate_dates[date] <- right_dates[i] 
}
```



#Pie chars
```{r Pie chart total sentiment both parties, fig.cap="Fig. 8: Total Sentiment since 1992 - Both Parties"}
#Pie chart Democrats - over all sentiment 
sentiment_debate_words %>%
filter(grepl("[RD]", party)) %>% 
count(sentiment) %>%  
plot_pie_chart() +
labs(title = "Fig. 9: Borth Parties' Total Sentiment Since 1992 - BING",
       x = "",
       y = "",
       fill = "sentiments")

save_plot_pdf("fig_9")
```


```{r pie chart combined sentiment all years, fig.cap="Fig. 15: Combined sentiment for each year - BING"}
#All 
sentiment_debate_words %>%
  group_by(year, sentiment) %>% 
  tally() %>% 
  plot_pie_chart() +
  facet_wrap(~year, ncol = 7) +
  labs(title = "Fig. 15: Total sentiment by year - BING",
       x = "",
       y = "",
       fill = "sentiments")

save_plot_pdf("fig_15")
```

```{r comparison cloud both parties, fig.cap="Fig. 21: Comparison cloud for both parties"}
sentiment_debate_words %>% 
  filter(grepl("[RD]", party)) %>% 
  count(word, sentiment, sort = TRUE) %>%   
  acast(word ~ sentiment, value.var = "n", fill = 0) %>% 
  create_comparison_cloud(title_value = "Sentiment Comparison Cloud - Both Parties")

save_plot_pdf("fig_22")
```

#columns sentiment:
```{r bar chart R, fig.cap="Fig. 11: Sentiment distribution since 1992 for Republicans - AFINN"}
sentiment_debate_words %>%
  filter(party == "R") %>% 
  count(value) %>%
  ggplot(aes(x = value, y = n)) +
  geom_col() +
    labs(title = "Fig. 11: Sentiment distribution since 1992 for Republicans - AFINN",
       x = "sentiment",
       y = "count")

save_plot_pdf("fig_11")
```


##From text
```{r Pie chart Republicans since 1992, fig.cap="Fig. 7: Total Republican sentiment since 1992"}
#Pie chart Republicans - over all sentiment
sentiment_debate_words %>% 
filter(party == "R") %>% 
count(sentiment, sort = TRUE) %>%  
plot_pie_chart() +
labs(title = "Fig. 7: Republicans Total Sentiment Since 1992 - BING",
       x = "",
       y = "",
       fill = "sentiments")

save_plot_pdf("fig_7")
```

Republicans appears to have been largely postive combined. With around 60% of the counted words containing a postive sentiment. Is this also true for the democrats?
```{r Pie chart Democrats since 1992, fig.cap="Fig. 8: Total democratic sentiment since 1992"}
#Pie chart Democrats - over all sentiment 
sentiment_debate_words %>%
filter(party == "D") %>% 
count(sentiment) %>%  
plot_pie_chart() +
labs(title = "Fig. 8: Democrats Total Sentiment Since 1992 - BING",
       x = "",
       y = "",
       fill = "sentiments")

save_plot_pdf("fig_8")
```

Let's try to look the top 30 words that are preceeded by a negation word, and see how much they distort the overall sentiment:
```{r plot contribution, fig.cap="Fig. 16: Top 30 words that distorts the total sentiment most"}
all_debates_bigrams %>% 
  count(word1, word2, value, sort = TRUE) %>% 
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  head(30) %>% 
  mutate(word2 = reorder(word2, contribution)) %>%
  arrange(desc(n)) %>% 
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Fig. 16: Sentiment distortion by word - AFINN",
       x = "Total sentiment value",
       y = "Word")

save_plot_pdf("fig_16")
```
"True" appears to be one of the most common negation words which makes the overall sentiment look more positive than it actually is. However there are also word in the opposite directed, such as "doubt" which makes the sentiment more negative than they actually are.


Since this is the case, maybe we should conclude the analysis by making a commonality cloud where we can see overlap in their word usage. I'm using the data from the sentiment analysis in order to get the same words, but rather than using traditional sentiment, I will here rather look at their frequency. The higher frequency the larger the overlap between the two parties. I'm using the function *commonality.cloud*, from the package *worcloud* (Fellows, n.d.) 

```{r}
sentiment_debate_words %>% 
  filter(grepl("[RD]", party)) %>%
  count(party, word, sort = TRUE) %>% 
  acast(word ~ party, value.var = "n", fill = 0) %>% 
  commonality.cloud(random.order = FALSE,
                   scale = c(2, 0.2)) +
  title(main = "Fig. 4: Commonality Cloud - Both Parties")

save_plot_pdf("test")
```
As previously noted, their positive sentiment appears to be the where we find the most overlap between the parties, this suggest, that each party have very different opinions about what negative phenomenons are worth discussing. 


```{r comparison republicans, fig.cap="Fig. 21: Comparison cloud for Democrats"}
sentiment_debate_words %>% 
  filter(party == "R") %>% 
  count(word, sentiment, sort = TRUE) %>% 
  acast(word ~ sentiment, value.var = "n", fill = 0) %>% 
  create_comparison_cloud(title_value = "Fig. 21: Sentiment Comparison Cloud - Republicans")

save_plot_pdf("fig_21")
```
What is interesting, is how much overlap there actually is between both parties. They tend to use the same words a lot, and the only real difference in their use of loaded words is when we look at less common sentiments such as "terror", which is used by republicans" and "crisis", which is more commenly used by democrats. 

Let's try to make a comparison cloud across all debates since 1992 for democrats and republicans in order to se what they usually talk about:
```{r comparison Democrats, fig.cap="Fig. 20: Comparison cloud for Democrats"}
sentiment_debate_words %>% 
  filter(party == "D") %>% 
  count(word, sentiment, sort = TRUE) %>%   
  acast(word ~ sentiment, value.var = "n", fill = 0) %>% 
  create_comparison_cloud(title_value = "Fig. 20: Sentiment Comparison Cloud - Democrats")
 
save_plot_pdf("fig_20")
```
Again we see the same themes. Democrats usually talk about "crisis", "problem", "wrong", debt", "lost" and "mistake". Meanwhile their most commonly used positive words are "like", "good", "important", "better" and "support".



Let's try to make a comparison cloud across all debates since 1992 for democrats and republicans in order to se what they usually talk about:
```{r comparison Democrats, fig.cap="Fig. 20: Comparison cloud for Democrats"}
sentiment_debate_words %>% 
  filter(party == "D") %>% 
  count(word, sentiment, sort = TRUE) %>%   
  acast(word ~ sentiment, value.var = "n", fill = 0) %>% 
  create_comparison_cloud(title_value = "Fig. 20: Sentiment Comparison Cloud - Democrats")
 
save_plot_pdf("fig_20")
```
Again we see the same themes. Democrats usually talk about "crisis", "problem", "wrong", debt", "lost" and "mistake". Meanwhile their most commonly used positive words are "like", "good", "important", "better" and "support".


```{r comparison republicans, fig.cap="Fig. 21: Comparison cloud for Democrats"}
sentiment_debate_words %>% 
  filter(party == "R") %>% 
  count(word, sentiment, sort = TRUE) %>% 
  acast(word ~ sentiment, value.var = "n", fill = 0) %>% 
  create_comparison_cloud(title_value = "Fig. 21: Sentiment Comparison Cloud - Republicans")

save_plot_pdf("fig_21")
```

Let's try to look at the republicans:
```{r sentiment word cloud republicans, fig.cap="Fig. 19: Republican word cloud. The size of each word is the count, the color is the sentiment"}
sentiment_debate_words %>%
  filter(party == "R") %>% 
  count(year, word, value, sort = TRUE)  %>% 
  group_by(year) %>% 
  slice_max(order_by = n,n= 50) %>%  
  plot_wordclouds(color_value = value) +
  scale_color_gradient(low = "red", high = "green" ) +
  facet_wrap(~year, ncol = 3) +
  labs(title = "Fig. 19: Republicans' sentiment value and word count each year")

save_plot_pdf("fig_19")
```

```{r}
stop_words_removed %>% 
  filter(party == "D") %>%
  count(year, word, sort = TRUE) %>% 
  group_by(year) %>% 
  slice_max(order_by = n, n = 20) %>% 
  plot_wordclouds() +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
  facet_wrap(~year) +
  labs(title = "Fig. 1: Stop Word Filtering - Democrats")
  
save_plot_pdf("test")
```


```{r republicans word cloud stop word filtering, fig.cap="Fig. 2: Stop Word Filtering - Republicans"}
stop_words_removed %>% 
 filter(party == "R") %>% 
  count(year, party, word, sort = TRUE) %>% 
  group_by(year) %>% 
  slice_max(order_by = n, n = 20) %>% 
  plot_wordclouds() +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
  facet_wrap(~year, ncol = 3, scales = "free") +
   labs(title = "Fig. 2: Stop Word Filtering - Republicans")

save_plot_pdf("fig_2")
```

To some extent yes


### Distortion
One important thing to remember, is that the sentiment analysis does not provide context for words (Silge & Robinson, XXXX, FIXME), Negation words such as "not", "never", "without" etc. changes the sentiment to the opposite. For example the phrase "I never liked" actually makes the word negative rather than positive. Luckily, I can use bigrams, to see how skewed the sentiment analysis is to either a positive or negative sentiment (ibid.)
I start out by creating a vector of so *negation words* these are words that flips the sentiment of the following word. 

```{r bigrams negation words}
negation_words <- c("not", "no", "never", "without", "don't", "can't", "cannot", "doesn't", "couldn't", "shouldn't")
```

I then remove debates from before  1992 in the tibble *all_debates*, unnest by *bigrams* rather than words, and seperate each word in a bigram into two columns *word1* (first word) and *word2* (the second word). Then, using *filter*, I remove words that aren't preceded by a negation word. Lastly, I inner_join the afinn lexicon by the column *word2*.  The result is saved in an object called *all_debates_bigrams*.
```{r bigrams }
all_debates_bigrams <- all_debates %>% 
  filter (year >= 1992) %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(word1 %in% negation_words) %>% 
  inner_join(get_sentiments("afinn"), by = c(word2 = "word"))

#save_plot_pdf("fig_16")
```

Let's try and count by year in order to see, how each year have been affected by negation words:
```{r sentient distortion by year, fig.cap="Fig. 17: How each year's sentiment is distorted"}
all_debates_bigrams %>% 
  count(word1, word2, year, value, sort = TRUE) %>% 
  mutate(contribution = n * value) %>% 
  group_by(year) %>% 
  summarise(total = sum(contribution)) %>% 
  ggplot(aes(x = year, y = total), stat = "identity") +
  geom_col() +
  scale_x_continuous(breaks=seq(1992, 2020, by = 4)) +
  scale_y_continuous(breaks = seq(0, 70, by = 10)) +
  labs(title = "Fig. 8: Sentiment distortion each year- AFINN",
       x = "Year",
       y = "Sentiment value")


save_plot_pdf("fig_8")
```
All debates since 1992, are actually skewed towards a more positive result. This is especially pronounced in 2008, where the overall sentiment, according to the afinn lexicon, is at least 75 points more positive than it should be. What is also interesting is that, the years that are most skewed (2008, 2016 and 2020) are also the years where the previous sentiment analysis showed the most negative sentiment across both parties. This means that these, quite negative debates, are actually even more negative, than my first analysis showed.
This  sugggest that both parties have used an increasingly negative vocabulary since 2008. 

Since this is the case, maybe we should conclude the analysis by making a commonality cloud where we can see overlap in their word usage. I'm using the data from the sentiment analysis in order to get the same words, but rather than using traditional sentiment, I will here rather look at their frequency. The higher frequency the larger the overlap between the two parties. I'm using the function *commonality.cloud*, from the package *worcloud* (Fellows, n.d.) 

```{r}
sentiment_debate_words %>% 
  filter(grepl("[RD]", party)) %>%
  count(party, word, sort = TRUE) %>% 
  acast(word ~ party, value.var = "n", fill = 0) %>% 
  commonality.cloud(random.order = FALSE,
                   scale = c(2, 0.2)) +
  title(main = "Fig. 10: Commonality Cloud - Both Parties")

save_plot_pdf("fig_10")
```
As previously noted, their positive sentiment appears to be the where we find the most overlap between the parties, this suggest, that each party have very different opinions about what negative phenomenons are worth discussing. 
Again democrats appears to have almost the same sentiment as republicans. Bing however, is a binary dictionary. It therefore, does not tell us how positive or negative they are. But using afinn we can see their sentiment distribution. For this I'm using a bar-chart since those are more reliable when it comes displaying multiple values:

```{r Bar chart total sentiment since 1992 - afinn, fig.cap="Fig. 7: Sentiment distribution since 1992 for Both Parties - AFINN"}
sentiment_debate_words %>%
  filter(grepl("[RD]", party)) %>% 
  group_by(party) %>% 
   mutate(party = str_replace(party, "D", "Democrats")) %>% 
  mutate(party = str_replace(party, "R", "Republicans")) %>% 
  count(party, value) %>%
  ggplot(aes(x = value, y = n)) +
  geom_col() +
    labs(title = "Fig. 7: Sentiment Distribution For Both Parties - AFINN",
       x = "sentiment",
       y = "count") +
    facet_wrap(~party, ncol = 2, scales = "fixed")

save_plot_pdf("fig_7")
```
Interestingly it appears, that their sentiment distribution is largely the same for both parties. They have very few strongly negative or positive words, and primarily using words with a sentiment value of 2. 

Let's try to seperate between parties. This is done the same way as previously, but rather than calculating tf-idf based on year, I'm doing it based on party:

```{r tf_idf by party}
word_count_party <- debate_words %>% 
  count(party, word, sort = TRUE)

total_words_party <- word_count_party %>%
  group_by(party) %>% 
  summarise(total = sum(n))

word_count_party <- left_join(word_count_party, total_words_party)

word_tf_idf_party <- word_count_party %>% 
  bind_tf_idf(word, party, n)
```

Let's plot it:
```{r tf_idf by party all years, fig.cap="Fig. 4: TF-IDF for each party across all years"}
word_tf_idf_party %>% 
  group_by(party) %>% 
  slice_max(order_by = tf_idf, n = 20) %>% 
  plot_wordclouds(size_value = tf_idf, color_value = tf_idf) +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
  facet_wrap(~party, ncol = 2, scales = "free") +
  labs(title = "Fig. 4: TF-IDF for each party" )

save_plot_pdf("fig_4")
```
Interesingly Donald and McCain are some of the words with the highest TF-IDF for democrats, While repulicans have talked a lot about Clinton and Obama. Here it is also interesting to note, how bizarre the Independent's talking point are. They apparently talk alot about teams, potatoes, the diplomat Glaspie, bleeding and even shoes. It is beyond the scope of this exam to also investigate the independents rhetoric, but it might be interesting to investigate the independents rhetoric for another paper. 

Let's try to make a comparison cloud across all years in order to see most common words across all debates. This is done by filtering out rows, where the column *party* does not contain the string "R" or "D". "R" is then transformed in to "Republicans" and "D" is tranformed to "Democrats". This is done for aestetic reasons (otherwise the headings will be "R" and "D").  Then, using the function *acast* from the package *Reshape2* (Wickham, 2007), the output is transformed to a wide *term-document-matrix*, where each column represents either republicans or democrats and each row represents a word and how often they are uttered by a democratic and a republican candidate. If a word is mentioned 0 times by one of the parties, it will be filled with 0 rather than NA.

```{r comparsion cloud both parties, fig.cap="Fig. 2: Comparison cloud generated by stop word filtering"}
stop_words_removed  %>% 
  filter(grepl("[DR]", party)) %>% 
  mutate(party = str_replace(party, "D", "Democrats")) %>% 
  mutate(party = str_replace(party, "R", "Republicans")) %>% 
  count(party, word, sort = TRUE) %>% 
  acast(word ~ party, value.var = "n", fill = 0) %>%   
  create_comparison_cloud(color_values = c("Blue", "Red"))

save_plot_pdf(name ="fig_2")
```
Even when combining all years,

Some of the most mentioned word for republicans are "Obama" and "Clinton", while Democrats have talked a lot about "Mccain", "Donald", and "Bush". Bush and Clinton are understandable, they are political dynasties, with multiple presidential and vice-presidential candidates. However Obama only ran twice, and McCain only ran once. Therefore


For comparison cloud, I will make a custom function with a lot of default values for the parameters.
```{r plot comparison clouds function}
create_comparison_cloud <- function(data_set,  title_value = "", color_values = c("darkred", "darkgreen"), title_bg_colors = color_values, title_text_colors = c("white")) {
  data_set %>% 
  comparison.cloud(colors = color_values,
                   random.order = FALSE, 
                   scale = c(2, 0.2),
                   title.bg.colors = title_bg_colors,
                   title.colors = title_text_colors,
                   title.size = 1) +
title(main = title_value, line = 1)
  
}
```

## Combining Sentiment and wordclouds
Lastly, let's get an overview of which words they use, how often they are used, and their sentiment.
This can be achieved with a word cloud, where the size of each word is determined by the count, the color is determined by the sentiment (red = negative, green = postive) according to the "afinn" lexicon.
Let's start with the democrats:
```{r sentiment word cloud democrats, fig.cap="Fig. 5: Afinn Word cloud. The size of each word is the count, the color is the sentiment"}
sentiment_debate_words %>%
  filter(grepl("[DR]", party)) %>% 
  count(year, word, party, value, sort = TRUE) %>%
  group_by(year, party) %>% 
  slice_max(order_by  = n, n= 50) %>%    
  plot_wordclouds(color_value = value) +
  scale_color_gradient(low = "red", high = "green") +
    facet_wrap(~year + party) +
  labs(title = "Fig. 5: Both parties' sentiment and word count year")

save_plot_pdf("fig_5")
```

Interestingly the same words tends to be used across all debates. Words such as "Like", "important", "safe" and "better" are often used as positive words. The word "like" is especially problematic here, since it can be an adjective, a verb, a preposition and an adverb, which changes its sentiment, this suggest that the debates are even more negative than our previous analysis showed.
Moreover we see that words like "lost", "debt", "crisis", "problem", and "wrong" are often used as negative words. Moreover, we can see that especially 2020 contain a lot of red words which supports my previous analysis, which found that 2020 was an especially negative debate. Republicans tends to use the same positive words as democrats. However their negative words differ. They use words like "terror" "threat", and "bad" a lot more. Moreover, even though they do use the word "crisis" it is much less used by the republicans compared to the democrats. Moreover, the republicans appeared to us a lot more negative words in 2016, compared to 2020. 
What is interesting, is how much overlap there actually is between both parties. They tend to use the same words a lot, and the only real difference in their use of loaded words is when we look at less common sentiments such as "terror", which is used by republicans" and "crisis", which is more commonly used by democrats. 