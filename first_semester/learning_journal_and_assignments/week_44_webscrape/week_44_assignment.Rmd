---
title: 'Assignment week 44: Webscraping'
author: "Christoffer M. Kramer"
date: "30/10/2020"
output:
  word_document: default
  html_document: default
---

My repo: https://github.com/Digital-Methods-HASS/au590388_Christoffer_Kramer/tree/master/learning_journal_and_assignments/week_44_webscrape


**Take the repository at https://github.com/adivea/KilledbyPolice2020.git and depending on your familiarity with R, either** 

1. *adapt the webscraping example to scrape homicide data from FBI site and produce a meaningful report on how homicide trends evolve around US in relation to this urban unrest*

2. *use the rvest library to scrape data of your interest (football statistics in Wikipedia? global population by country in https://www.worldometers.info/world-population/population-by-country/ )*

3. *produce data visualisations that shed light on another interesting aspect of the police killing data*

I will do the second item on the list. I will scrape transcripts of presidential debates, since these will be usefull for my final project.

I start by loading the relevant libraries
```{r}
library(rvest) 
library(dplyr) 
library(tidyr)
library(stringr)
library(janitor)
library(tidyverse)
library(tidytext)
library(ggplot2)
```

Then I create af function for webscraping. This makes it possible to automate the webscraping process.
The parameter is the website I wish to scrape. 
The function reads the html elements of the website. It only reads the "p" elements, since this is where the text is located. It then parses the elements as text. Lastly, it saves the output in a vector, which is usefull for text-mining and data-cleaning.

```{r}
scrape_debates <- function(website) { #Create a function called scrape_debates
 p_html <-  read_html(website) %>%  #Create an object which contains the parameter "website" and is read as a html file
  html_nodes("p") %>% #Grab all <p> elements in the html file.
  html_text() #Parse the result as text.
 
vect_p_html <- c(p_html) #Save the output in a vector.
}
```

Let's test it:

```{r}
head(scrape_debates("https://www.debates.org/voter-education/debate-transcripts/2008-debate-transcript/"))
tail(scrape_debates("https://www.debates.org/voter-education/debate-transcripts/2008-debate-transcript/"))
```
It works, but I still have 40 transcripts that I need to scrape. I will, therefore, use a loop to automate the process.

The links to all transcripts are located here: https://www.debates.org/voter-education/debate-transcripts/
Therefore, I need to grab all the relevant links (without getting links from the nav-bar).

Luckily the links follow an easy structure. Every link is contained within an "a" element, which is inside of of "p" element, which inside a "blockqoute" element. 
So by navigating to the "blockqoute" element, I can grab the "child" (which is "p") and then navigate to the relevant "a" element and then select all a elements that contain a link (a href attribute).

```{r}
link_html <- read_html("https://www.debates.org/voter-education/debate-transcripts/") %>% 
 html_nodes("blockquote") %>% #Grab the blockqoute elements
  html_children() %>% #Navigate to the blockqoute's children.
  html_node("a")  %>% #Grab <a> elements
  html_attr("href") #Grab <a> elements which contains a hyperlink.

vect_link <- c(link_html) #save the output in a vector
```

Let's see if it works.
```{r}
vect_link
```
It works. Every element in this vector is a link to a relevant debate transcript.
There are two NA's, which (I believe) is from two "p" elements, that does not contain any "a" elements. 
This is easily dealt with and is, therefore, not a problem. 

Now I just need to automate the process. This is done with a loop which will create an object for each transcript.
The loop will not go through NA values.
I start by creating an object called "debate_names", this will contain the name of each debate object.
Then I loop through each element in my previous vector "vect_link". 
I then create an object called "name". This object contains a substring from the current link. This is used for naming the objects. If the link does not match the regex, it will just contain the full link.
Then I create an object called website. It contains the first part of the url and the current link which are concatenated.
Then I push the name into my empty vector (debate_names).
I then create an object called "debate". This contains the scraped data.
The debate object is then transformed to a tibble, which is easier to text-mine.
Lastly, I create an unique object with the function assign(). This object has the same name as the name in the vector "debate_names", and it contains the value of the object "debate".
```{r, include=FALSE}
#For my final project, I had to remove these in order to make the web-scraping work. This is because the 2020 debates weren't included when I wrote this.
vect_link[1] <- "/voter-education/debate-transcripts/september-29-2020-debate-transcript/"
vect_link[2] <- "/voter-education/debate-transcripts/vice-presidential-debate-at-the-university-of-utah-in-salt-lake-city-utah/"
vect_link[3] <- "/voter-education/debate-transcripts/october-22-2020-debate-transcript/"

```

```{r}
debate_names = NULL #Create an empty object

for (link in vect_link[!is.na(vect_link)]) { #For every link in the vector "vect_link" (Na excluded")
 
  name <- str_extract(link, "[A-Za-z]+-\\d+-\\d+") #Create an object called name, set the value to be the regex match.
  
  if(is.na(name)) { #If name is NA
    name <- str_extract(link, ".+") #Set name's value to be the same as the current link
  }

  debate_names = append(debate_names, paste("debate_",name, sep = "")) #Add the object "name" to the vector "debate_names"
  
  website <- paste0("https://www.debates.org/", link) #Create an object called "website".
  debate <- scrape_debates(website) #Scrape data from the website and save it in an object called "debate"
  debate <- tibble(line = 1:length(debate), text = debate) #Make "debate" a tibble, so it can be text-mined
  assign(paste("debate_", name, sep = ""), debate) #Create an object with the same name as the element in "debate_names" which contains the value of debate.
  

}
```

By now I should have the name for each object in the vector "debate_names", let's check the vector out.

```{r}
debate_names
```

It appears to be working. Now I should be able to access each debate by just typing the name of one of the objects above.

```{r}
`debate_october-11-1992`
```


It works! Now each debate is scraped, saved in an object and is in a dataframe. Let's do a very simple text-analysis on one of the debates. First, I need to get a list of stop words, which is included in the tidytext package. Then I  tokenize the debate by word so I can do an analysis. I do this with the function unnest_tokens. I then remove all stopwords from the debate with anti_join(). Then I count each word and sort it. Then I filter out all words that does not appear more than 30 times. Lastly, I pipe it directly in to a ggplot:

```{r}

data("stop_words")#Get all stopwords

 `debate_october-11-1984` %>% #Chose the debate of october 11th. 1984.
  unnest_tokens(word, text) %>% #Tokenize by word
  anti_join(stop_words) %>% #Remove stop words
  count(word, sort = TRUE) %>% #Count each word and sort the result.
     filter(n > 30) %>% #Remove words, that does not appear more than 30 times
  ggplot(aes(word, n)) + #Make a ggplot
  geom_col() + #Make it a bar chart where the height represents the value
  coord_flip() #Flip the bar chart


```

As you can see, there is still a lot of cleaning to be done, but it is a great start. 