---
title: "Text Mining Turtorial"
author: "Christoffer M. Kramer"
date: "28/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Turtorial found here: https://www.tidytextmining.com/

#Chapter 1 - Tidy text format
Hver variabel er en kolonne, hver observation er en række, hver type af obersrvationel enhed er en tabel
Tidy text: One token per row. 
Token = En meningsfuld enhed af tekst. 
Tokenization = Splitte teksten til tokens. 
Token = Et ord, men kan også være en sætnming eller paragraf. 


```{r}
text <- c("Because I could not stop for Death -",
          "He kindly stopped for me -",
          "The Carriage held but just Ourselves -",
          "and Immortality")

text
```

For at analysere dette skal vi først have det i en dataramme:
```{r}
library(tidyverse)
library(dplyr)
text_df <- tibble(line = 1:4, text = text)

text_df
```

Vi skal have teksten ind i tokens og vi skal tranformere det til en tidy datastruktur.
```{r}
#install.packages("tidytext")
library(tidytext)
text_df %>%
  unnest_tokens(word, text)
```
Unnest_tokens(word, text) viser at vi vil bruge det på tekst og at vi vil lave tokens ud fra ord.
Punktummer fjernes. Alt laves til lowercase, dette kan ændres med to_lower = FALSE

Gem bøgerne fra Jane Austen
```{r}
library(janeaustenr)
library(dplyr)
library(stringr)

original_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>%
  ungroup()

original_books
```
Tokenize
```{r}
tidy_books <- original_books %>%
  unnest_tokens(word, text)

tidy_books
```

Stopord kan fjernes med datasættet stopwords.
anti_join fjerne ord fra det datasæt.

```{r}
data(stop_words)

tidy_books <- tidy_books %>%
  anti_join(stop_words)
```
```{r}
tidy_books %>%
  count(word, sort = TRUE) 
```

Count bruges til at tælle de mest brugte ord. 

Vi kan nu visualisere med ggplot
```{r}
tidy_books %>%
  count(word, sort = TRUE) %>%
  filter(n > 600) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

##Gutenberg pakken
Giver adgang til offentlige tekster
Vi kan sammenligne ordfrekvens på tværs af tekster, her bruges "The time machine", "The war of the Worlds", "The invisible man" og "The island of doctor Moreau" Med Wells

```{r}
#install.packages("gutenbergr")
library(gutenbergr)

hgwells <- gutenberg_download(c(35, 36, 5230, 159))

tidy_hgwells <- hgwells %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

tidy_hgwells %>%
  count(word, sort = TRUE)
```
Hvad med Bronte?
```{r}
bronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))

tidy_bronte <- bronte %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

tidy_bronte %>%
  count(word, sort = TRUE)

```

Lad os udregne frekvensen og omdanne datasættet, så det kan bruges til at plotte og sammenligne bøgerne.

```{r}
library(tidyr)

frequency <- bind_rows(mutate(tidy_bronte, author = "Brontë Sisters"),
                       mutate(tidy_hgwells, author = "H.G. Wells"), 
                       mutate(tidy_books, author = "Jane Austen")) %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(author, word) %>%
  group_by(author) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(author, proportion) %>% 
  gather(author, proportion, `Brontë Sisters`:`H.G. Wells`)
```

Here bruges str_extract, da datasættet benytter underscore til at angive italics.
Vi sammenligner herefte Bronte med Austen, mens Wells Sammenlignes med Austen
```{r}

library(scales)

# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = proportion, y = `Jane Austen`, color = abs(`Jane Austen` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Jane Austen", x = NULL)
```
Desto tættere ordene er på linjen, desto mere overlap er der mellem ordene. Bronte har mere tilfælles med Austen end Wells.

Hvor korrelerede er ordfrekvensen mellem austen og Bronte?

```{r}
cor.test(data = frequency[frequency$author == "Brontë Sisters",],
         ~ proportion + `Jane Austen`)
```

Hvad med Austen og Wells?
```{r}
cor.test(data = frequency[frequency$author == "H.G. Wells",], 
         ~ proportion + `Jane Austen`)
```

#Chapter 2 - Sentiment Analyse
Kaldes Opinion mining. 

En metode til dette, er at se på teksten som en kombination af individuelle ord. Herefter ser på på hele teksten som summen af meningen i de individuelle ord. 
3 generelle datasæt med sentiment leksikonner 
-Afin
-bing
-Nrc

get_sentiments() finder leksikonnet

```{r}
#install.packages("textdata")
library(tidytext)
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
```
Disse ord findes me crowdsource. De er derfor ikek altid passende med bestemte tekstyper (f.eks. 200 år gammel fiktion).

Disse tager ikke kvalificerer med f.eks. "no good".
PÅ store datasæt kan det ofte ende med at det hele i gennemsnit bliver 0. Men med mindre stykker f.eks. sætninger eller paragraffer så er det mere præcist og mere ladet.

Først laves det til tidy format

```{r}
library(janeaustenr)
library(dplyr)
library(stringr)

tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", 
                                                 ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)

```
Find ord, der er klassificeret som Joy
```{r}
library(tidytext)
library(textdata)

nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```

 Vi kan se, hvordan sentimentet har ændret sig over tid. Her vurderes ud fra 80 linjer ad gangen. Dette gøres ved at dividerer linjenummer med 80 ( %% )
 
```{r}
library(tidyr)

jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```
 
 Nu kan vi plotte:
```{r}
library(ggplot2)

ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```
 
 Vi kan sammenligne de forskellige leksikonner for at se, hvordan de påvirker vores resultater.
 V bruger kun bogen "PRide and Prejudice"
```{r}
pride_prejudice <- tidy_books %>% 
  filter(book == "Pride & Prejudice")

pride_prejudice
```
 
```{r}
 afinn <- pride_prejudice %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(pride_prejudice %>% 
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing et al."),
                          pride_prejudice %>% 
                            inner_join(get_sentiments("nrc") %>% 
                                         filter(sentiment %in% c("positive", 
                                                                 "negative"))) %>%
                            mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```
 
 Lad os nu sammenbinde dem og plotte dem
```{r}
bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```
 
 NRC har meget højere værdier. De har altså alle sammen cirka samme struktur, men deres absolutte værdier er væsentligt fortskellige (Bing gør nogle sektioner værsentligt mere negative end de andre, mens NRC er væsentlig mere positiv).
 Hvorfor? Vi kan prøve at se, hvor mange positive og negative ord der er i hvert datasæt:
 
```{r}
get_sentiments("nrc") %>% 
     filter(sentiment %in% c("positive", 
                             "negative")) %>% 
  count(sentiment)

get_sentiments("bing") %>% 
  count(sentiment)

```
 
 Bing har væsentligt flere negative ord, og væsentlig færre positive ord, sammenlignet med NRC, derfor er NRC mere positv.
 
Vi kan tælle ordene, og se deres ladning.
```{r}
bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts
```

Dette  kan visualiseres:

```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```
Der er en fejl i sentimentet. Miss er ikke negativt, det er en måde at tiltale hovedpersonen på. Vi kan fjerne dette ord, ved at tilføje det til stopordslisten.

```{r}
custom_stop_words <- bind_rows(tibble(word = c("miss"), 
                                          lexicon = c("custom")), 
                               stop_words)

custom_stop_words
```

##Word clouds
```{r}
#install.packages("wordcloud")
library(wordcloud)

tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

Man kan sammenligne med comparison.cloud, dog kræver dette, at datarammen er en matrix, hvilket gøres med reshape2 acast().

```{r}
#install.packages("reshape2")
library(reshape2)

tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

##Mere end ord
Man kan også kigge på sætninger. Dette kan gøres med coreNLP, cleanNLP og sentimentr
Disse kigger på sætninger ikke ord og vurderer sentimentet.

```{r}
PandP_sentences <- tibble(text = prideprejudice) %>% 
  unnest_tokens(sentence, text, token = "sentences")
PandP_sentences$sentence[2]

```

Man kan også skabe tokens vha. et regex. Nedenunder opdeler vi datarammen pr. kapitel.

```{r}
austen_chapters <- austen_books() %>%
  group_by(book) %>%
  unnest_tokens(chapter, text, token = "regex", 
                pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
  ungroup()

austen_chapters %>% 
  group_by(book) %>% 
  summarise(chapters = n())
```

```{r}
bingnegative <- get_sentiments("bing") %>%  #Vælg fra bing datasættet
  filter(sentiment == "negative") #Brug kun negative ord

wordcounts <- tidy_books %>% #Lav nyt objekt med tidy_books objektet der allered er tokeniseret
  group_by(book, chapter) %>%  #Gruper ud fra bog og kapitel.
  summarize(words = n()) #LAv en summary med antallet af negative ord.

tidy_books %>%
  semi_join(bingnegative) %>% #Join med bingnegative
  group_by(book, chapter) %>% #Gruper ud fra bog og kapitel.
  summarize(negativewords = n()) %>% #Sum ud fra negative ord.
  left_join(wordcounts, by = c("book", "chapter")) %>% #Indsæt "wordcounts" ud fra bog og kapitel.
  mutate(ratio = negativewords/words) %>% #Lave en kolonne, der hedder ratio, der dividerer antallet af negative ord med antallet af ord i alt.
  filter(chapter != 0) %>% #Fjern kapitler, hvor der ikke er negative ord.
  top_n(1) %>%
  ungroup()
```
#Chapter 3 - Analysing word and document frequency
En liste med stopord er ikke en ret sofistikeret måde at måle ordene. En anden metode er inverse dokument frequency. Den bruges til at vurdere hvor vigtigt et ord er ud fra et korpus.
Lad os beregne det samlede antal ord i Jane Austens noveller.

```{r}
library(dplyr)
library(janeaustenr)
library(tidytext)

book_words <- austen_books() %>%
  unnest_tokens(word, text) %>%
  count(book, word, sort = TRUE)

total_words <- book_words %>% 
  group_by(book) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

book_words
```
Lad os dividere n med total for at se distributionen ud fra antallet af ord.
```{r}
library(ggplot2)

ggplot(book_words, aes(n/total, fill = book)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~book, ncol = 2, scales = "free_y")
```
De følger en zips fordeling.
VI kan undersøge dette, hvilken rank har det og hvad er dets frekvens?

```{r}
freq_by_rank <- book_words %>% 
  group_by(book) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total)

freq_by_rank
```

Dette kan plottes
```{r}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = book)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```

```{r}
rank_subset <- freq_by_rank %>% 
  filter(rank < 500,
         rank > 10)

lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)

```

```{r}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = book)) + 
  geom_abline(intercept = -0.62, slope = -1.1, color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```
Austen adskiller sig fra andre forfattere ved de lavt rangerende ord. 
Ideen med tf_idf er at finde ord, der bruges ofte, men ikke bruges hele tiden. Det er simpelthen en måde at fjerne stopord på. Lad os se på de ord, som har en lav tf_idf:

```{r}
book_words <- book_words %>%
  bind_tf_idf(word, book, n)

book_words

```

Lads os se på ord med en høj tf_idf:

```{r}
book_words %>%
  select(-total) %>%
  arrange(desc(tf_idf))
```

Lad os visualisere dette:

```{r}
book_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(book) %>% 
  top_n(15) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = book)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~book, ncol = 2, scales = "free") +
  coord_flip()

```
Vi kan se at hun skriver på samme måde, hovedsforskellen er altså hendes navneord. Dette er formålet med idf, det identificerer ord, der er vigtige i ud dokument ud fra en kollektion af dokumenter.

Lad os prøve med fysik tekster

```{r}
library(gutenbergr)
physics <- gutenberg_download(c(37729, 14725, 13476, 30155), 
                              meta_fields = "author")

physics_words <- physics %>%
  unnest_tokens(word, text) %>%
  count(author, word, sort = TRUE)

physics_words
```

Lad os plotte det:

```{r}
#install.packages("forecast")
library(forcats)

plot_physics <- physics_words %>%
  bind_tf_idf(word, author, n) %>%
  mutate(word = fct_reorder(word, tf_idf)) %>%
  mutate(author = factor(author, levels = c("Galilei, Galileo",
                                            "Huygens, Christiaan", 
                                            "Tesla, Nikola",
                                            "Einstein, Albert")))

plot_physics %>% 
  group_by(author) %>% 
  top_n(15, tf_idf) %>% 
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(word, tf_idf, fill = author)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~author, ncol = 2, scales = "free") +
  coord_flip()
```
K bliver brugt meget i Einstens teori. hvorfor?

```{r}
library(stringr)

physics %>% 
  filter(str_detect(text, "_k_")) %>% 
  select(text)
```

Den skal altså rengøres, da den bruger _ til at opdele. Herudover erAB, og RC stråler, cirkler og vinkler.

```{r}
physics %>% 
  filter(str_detect(text, "RC")) %>% 
  select(text)
```

Vi tilføjer derfor ekstra stopord og plotter igen:

```{r}
mystopwords <- tibble(word = c("eq", "co", "rc", "ac", "ak", "bn", 
                                   "fig", "file", "cg", "cb", "cm",
                               "ab", "_k", "_k_", "_x"))

physics_words <- anti_join(physics_words, mystopwords, 
                           by = "word")

plot_physics <- physics_words %>%
  bind_tf_idf(word, author, n) %>%
  mutate(word = str_remove_all(word, "_")) %>%
  group_by(author) %>% 
  top_n(15, tf_idf) %>%
  ungroup() %>%
  mutate(word = reorder_within(word, tf_idf, author)) %>%
  mutate(author = factor(author, levels = c("Galilei, Galileo",
                                            "Huygens, Christiaan",
                                            "Tesla, Nikola",
                                            "Einstein, Albert")))

ggplot(plot_physics, aes(word, tf_idf, fill = author)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~author, ncol = 2, scales = "free") +
  coord_flip() +
  scale_x_reordered()

```
#Chapter 4 - Relationships between words: n-grams and correlations

Mange interessante tekstanalyser handler ikke kun om individuelle ord, dokumenter og sentimenter men også relationerne mellem ordene. Hvilke ord følger hinanden og hvilke ord opstår i samme dokumenter?

Vi kan tokenisere ud fra n-gram, hvor vi ser på, hvilke ord, der ofte er efterfølger til et andet ord og bygge en model.

Vi vælger tokens = ngrams og sætter ngrams n til 2 (bigrams), hvilket handler om, hvor mange efterfølgende ord vi ser på:

```{r}
austen_bigrams <- austen_books() %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

austen_bigrams
```

Lad os sortere og se de mest anvendte bigrams:

```{r}
austen_bigrams %>%
  count(bigram, sort = TRUE)
```

Dette er stopord, vi kan bruge seperate(), som separere ordene ud fra en delimiter. Herefter benytte vores stopordsliste på den:

```{r}
bigrams_separated <- austen_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts
```

Vi kan sammensætte ordene igen med unite()

```{r}
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united
```


Vi kan også lave trigrammer med 3 ord osv.: 

```{r}
austen_books() %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  count(word1, word2, word3, sort = TRUE)

```

Lad os antage at vi er interessert i de mest nævnte gader, her er bigrams gode:

```{r}
bigrams_filtered %>%
  filter(word2 == "street") %>%
  count(book, word1, sort = TRUE)
```

Man kan lave en idf med bigrams:

```{r}
bigram_tf_idf <- bigrams_united %>%
  count(book, bigram) %>%
  bind_tf_idf(bigram, book, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf

```

Bigrammer er bedst til store datasæt, da de er relativt sjældne sammenlignet med enkelte ord. 

##Sentiment analyse med n-grammer
Man kan bruge bigrammer til at se konteksten, hvis ordet "not" er foran et ord, så er det negativt:

```{r}
bigrams_separated %>%
  filter(word1 == "not") %>%
  count(word1, word2, sort = TRUE)
```

Lad os se på AFIN 
```{r}
AFINN <- get_sentiments("afinn")

AFINN
```

Vi kan se, hvilke ord, der oftest efterfølges af "not".

```{r}
not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, value, sort = TRUE)

not_words
```

We kan se, hvilke ord, der har bidraget mest i den ene eller anden retning (altså de har lavet fejl):

```{r}
library(ggplot2)

not_words %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * value, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment value * number of occurrences") +
  coord_flip()
```
Not Like og Not help har været med til at bidrage med misvisende information, det får teksten til at se mere positiv ud end den er.

Vi kan lave en række ord, der ændrer betydningen for det efterfølgende ord:

```{r}
negation_words <- c("not", "no", "never", "without")

negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, value, sort = TRUE)

negated_words
```

Vi kan herefter se, hvordan hvert bigram har haft en indflydelse:

```{r}
negated_words %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * value, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"negation\"") +
    ylab("Sentiment value * number of occurrences") +
  coord_flip()
```
Interessant nok kan vi se at "doubt" trækker væsentligt i den anden retning.

##Visualiser netværket af bigrammer
Man kan se relationerne mellem ordene samtidigt, med et netværk. Her bruges igraph.

```{r}
#install.packages("igraph")
library(igraph)

# original counts
bigram_counts
```
Ud fra N kan vi vælge kun at se de mest almindelige kombinationer: 
```{r}
# filter for only relatively common combinations
bigram_graph <- bigram_counts %>%
  filter(n > 20) %>%
  graph_from_data_frame()

bigram_graph
```
Man kan bruge ggraph sammen med igrah for at lave visualiseringer. Man tilføjer tre lag (Ligesom ggplot bruger to akser, edge_lin, node_point, node_text:

```{r}
#install.packages("ggraph")
library(ggraph)
set.seed(2017)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```
Lad os gøre den tydeligere. Vi tilføjer æstetikken edge_ALFA som ændrer transperansen ud fra hvor stærk relationen er. Vi tilføjer hvilken retning ordene flydr med grid::arrow() og end_cap som fortæller at vi skal stoppe inden vi rammer noden. Vi roder med options for at ændre forverne. Vi tilføjer et tema theme_void
```{r}
set.seed(2016)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```
Det er faktisk muligt at sammensætte alle relationerne i en tekst.

##Bigrammer i andre tekster
Vi har lavet en del arbejde for at bruge bigrammer. Vi kan derfor lave en funktion, der kan genbruges. Denne funktion tæller antallet ag biogrammer.

```{r}
count_bigrams <- function(dataset) {
  dataset %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(!word1 %in% stop_words$word,
           !word2 %in% stop_words$word) %>%
    count(word1, word2, sort = TRUE)
}

```

Vi kan herefter lave en, der visualiserer for os:

```{r}
visualize_bigrams <- function(bigrams) {
  set.seed(2016)
  a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
  bigrams %>%
    graph_from_data_frame() %>%
    ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()
}

```

Lad os prøve ovenstående med Biblen:

```{r}
library(gutenbergr)
kjv <- gutenberg_download(10)
```

```{r}
library(stringr)

kjv_bigrams <- kjv %>%
  count_bigrams()

# filter out rare combinations, as well as digits
kjv_bigrams %>%
  filter(n > 40,
         !str_detect(word1, "\\d"),
         !str_detect(word2, "\\d")) %>%
  visualize_bigrams()
```
##Widyr
Det er i nogle tilfælde relevant at finde det på ord, der måske ikke er nærliggende, men kommer på tværs af dokumenter eller i samme sætninger. 
Her skal man først lave dataen om til en matrix.

Vi kan f.eks. se, hvor mange ord, der kommer inden for samme sektion (10 linjer)

```{r}
austen_section_words <- austen_books() %>%
  filter(book == "Pride & Prejudice") %>%
  mutate(section = row_number() %/% 10) %>%
  filter(section > 0) %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% stop_words$word)

austen_section_words
```
Vi kan nu se, hvilke ord, der oftest optræder i samme sektion, på tværs af sektionerne
```{r}
#install.packages("widyr")
library(widyr)

# count words co-occuring within sections
word_pairs <- austen_section_words %>%
  pairwise_count(word, section, sort = TRUE)

word_pairs
```
Darcy og Elisabeth opræder oftest sammen. 
Hvilke ord optræder Darcy oftest med?
```{r}
word_pairs %>%
  filter(item1 == "darcy")
```

Vi kan undersøge korrelationen. Det vil sige, hvor ofte ord optræder sammen relativt til hvor ofte de optræder alene. Dette er mere brugbart, da de hovedpersonerne selvfølgelig vil optræde mest sammen.

Vi kan bruge en phi koefficient til dette. Her bruges funktionen: pairwise_cor()

```{r}
# we need to filter for at least relatively common words first
word_cors <- austen_section_words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, section, sort = TRUE)

word_cors
```

Vi kan f.eks. undersøge, hvilke ord, der er mest korreleret til "pounds":
```{r}
word_cors %>%
  filter(item1 == "pounds")
```

Lad os vælge interessante og og find ord, der er associeret med disse:

```{r}
word_cors %>%
  filter(item1 %in% c("elizabeth", "pounds", "married", "pride")) %>%
  group_by(item1) %>%
  top_n(6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip()

```
Vi kan bruge et netværk til at visualisere:

```{r}
set.seed(2016)

word_cors %>%
  filter(correlation > .15) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```
Her er ordene altså lineærer. Der er ingen pile. De optræder blot i samme sektioner, ikke lige efter hinanden i en given rækkefølge.

## Chapter 5: Konverter til og fra tidy-formater
Mange værktøjer er ikke lavet til tidy data. Og mange udsender ikke tidy data. Derfor skal vi kunne konvertere til og fra dette dataformat.

De fleste benytter document-term matrix
-Hver række = et dokument
- hver kolonne = et term
- hver værdi = antallet af gange termet fremgår.

tidy() transformerer dtm til tidy datarammer.  kommer fra broom pakken.
cast() laver tidy til matrix. cast_sparse() konverterteres til en sparse, cast_dtm() og cast_dfm()

Atiklerne her er dtm format:

```{r}
#install.packages("tm")
#install.packages("topicmodels")
library(topicmodels)
library(tm)

data("AssociatedPress", package = "topicmodels")
AssociatedPress
```

Denne dtm er 99% sparse, dvs. at 99% er termerne er 0.
Vi kan se termerne med terms() functionen

```{r}
terms <- Terms(AssociatedPress)
head(terms)
```

Vi kan omdanne disse til et tidy format med tidy()

```{r}
library(dplyr)
library(tidytext)
ap_td <- tidy(AssociatedPress)
ap_td

```
Tidy versionen har ingen rækker hvor count er 0.
Vi kan herefter lave tidy analyser

```{r}
ap_sentiments <- ap_td %>%
  inner_join(get_sentiments("bing"), by = c(term = "word"))

ap_sentiments
```

Vi kan nu visualisere:

```{r}
library(ggplot2)

ap_sentiments %>%
  count(sentiment, term, wt = count) %>%
  ungroup() %>%
  filter(n >= 200) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(term = reorder(term, n)) %>%
  ggplot(aes(term, n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  ylab("Contribution to sentiment") +
  coord_flip()
```

##Tidy dfm objekter
VI kan også bruge det på dfm (document-feature matrix), såsom her:

```{r}
#install.packages("quanteda")
library(quanteda)
data("data_corpus_inaugural", package = "quanteda")
inaug_dfm <- quanteda::dfm(data_corpus_inaugural, verbose = FALSE)

inaug_dfm
```

Igen kan tidy bruges:

```{r}
inaug_td <- tidy(inaug_dfm)
inaug_td

```

Lad os prøve at benytte tf_idf på denne:

```{r}
inaug_tf_idf <- inaug_td %>%
  bind_tf_idf(term, document, count) %>%
  arrange(desc(tf_idf))

inaug_tf_idf
```

Man bruger complete() hvis man vil inkluderer tilfælde hvor noget kommer 0 gange:

```{r}

library(tidyr)

year_term_counts <- inaug_td %>%
  extract(document, "year", "(\\d+)", convert = TRUE) %>%
  complete(year, term, fill = list(count = 0)) %>%
  group_by(year) %>%
  mutate(year_total = sum(count))
```

Lad os  visualisere:

```{r}
year_term_counts %>%
  filter(term %in% c("god", "america", "foreign", "union", "constitution", "freedom")) %>%
  ggplot(aes(year, count / year_total)) +
  geom_point() +
  geom_smooth() +
  facet_wrap(~ term, scales = "free_y") +
  scale_y_continuous(labels = scales::percent_format()) +
  ylab("% frequency of word in inaugural address")
```

##Tidy til matrix
Mange værktøjer forventer matrixer. Her kan cast() bruges:
```{r}
ap_td %>%
  cast_dtm(document, term, count)
```

```{r}
ap_td %>%
  cast_dfm(document, term, count)
```
Nogle kræver blot en sparse matrix:
```{r}
#install.packages("Matrix")
library(Matrix)
library(tidytext)
# cast into a Matrix object
m <- ap_td %>%
  cast_sparse(document, term, count)

class(m)
```

Dette kan f.eks. gøres med Jane austens tekst:

```{r}
library(janeaustenr)

austen_dtm <- austen_books() %>%
  unnest_tokens(word, text) %>%
  count(book, word) %>%
  cast_dtm(book, word, n)

austen_dtm
```
MAn kan gemme dokumenter, nogle er lavet til at tilgå dokumenter:

```{r}
library(tm)
data("acq")
acq

```
Hvert dokument er gemt i en liste:

```{r}
# first document
acq[[1]]
```

Vi kan brugetidy til at gøre strukturen kompitabel med tidy værktøjer:
```{r}
acq_td <- tidy(acq)
acq_td
```

Nu kan vi arbejde, f.eks. finde de mest bruge ord:

```{r}

acq_tokens <- acq_td %>%
  select(-places) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words, by = "word")

# most common words
acq_tokens %>%
  count(word, sort = TRUE)
```

```{r}
# tf-idf
acq_tokens %>%
  count(id, word) %>%
  bind_tf_idf(word, id, n) %>%
  arrange(desc(tf_idf))
```

##Eksempel
Lad os finde artikler med de store techvirksomheder (virker ikke)
```{r}
# library(tm.plugin.webmining)
# library(purrr)
# library(tibble)
# library(tidyverse)
# library(tidytext)
# library(dplyr)
# 
# company <- c("Microsoft", "Apple", "Google", "Amazon", "Facebook",
#              "Twitter", "IBM", "Yahoo", "Netflix")
# symbol <- c("MSFT", "AAPL", "GOOG", "AMZN", "FB", "TWTR", "IBM", "YHOO", "NFLX")
# 
# download_articles <- function(symbol) {
#   WebCorpus(GoogleFinanceSource(paste0("NASDAQ:", symbol)))
# }
# 
# stock_articles <- tibble(company = company,
#                              symbol = symbol) %>%
#   mutate(corpus = map(symbol, download_articles))
```


For at analysere ovenstående med sentimentanalyse kan man benytte Loughran og Mcdonalds financial sentiment terms. Denne bruges til finansreporter. 

```{r}
#install.packages("loughran")

get_sentiments("loughran")
```

#Topic modelling
Man behandler hvert dokument som en rækker temaer, der kan overlappe.
Den mest benyttede model = Laten Dirichlet allocation.
Hvert dokument er en blanding af temaer, hvert tema er en blanding af ord.
Lad os tage fat i Associated press datasæt:

```{r}

library(topicmodels)

data("AssociatedPress")
AssociatedPress
```

LDA() bruges til at lave temamodeller:

```{r}
# set a seed so that the output of the model is predictable
ap_lda <- LDA(AssociatedPress, k = 2, control = list(seed = 1234))
ap_lda
```
Vi har nu lavet to temaer. Vi skal nu vælge per tema per ord sandsynlighed:

```{r}

library(tidytext)

ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics
```

Lad os tage de 10 mest benyttede ord pr. tema og visualisere dette:

```{r}
library(ggplot2)
library(dplyr)

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()
```
Lad os prøve, at finde de ord med størst forskel mellem temaerne:

```{r}

library(tidyr)

beta_spread <- ap_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

beta_spread
```

Hver mange dokumenter pr tema (kaldes gamma):

```{r}
ap_documents <- tidy(ap_lda, matrix = "gamma")
ap_documents

```
Vi kan se at dokument 6 næsten ikke har nogle ord fra topic 1, hvad sker der her?

```{r}
tidy(AssociatedPress) %>%
  filter(document == 6) %>%
  arrange(desc(count))
```

Det handler alstå om relationen mellem USA og diktatoren Manuel Noriega, derfor hører den til i tema 2.

#The great library heist
Når man bruger en statistisk metode, kan man bruge den på et simpelt datasæt, hvor man kender svaret.
Lad os antage at vi har fire bøger, der er ødelagte, som skal sammensættes igen vha. topic modelling.
Lad os først hente bøgerne:

```{r}

titles <- c("Twenty Thousand Leagues under the Sea", "The War of the Worlds",
            "Pride and Prejudice", "Great Expectations")

library(gutenbergr)

books <- gutenberg_works(title %in% titles) %>%
  gutenberg_download(meta_fields = "title")
```

Vi opdeler i kapitler, tokeniserer i ord, laver stopord og behandler hvert kapitel som et dokument:
```{r}
library(stringr)

# divide into documents, each representing one chapter
by_chapter <- books %>%
  group_by(title) %>%
  mutate(chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE)))) %>%
  ungroup() %>%
  filter(chapter > 0) %>%
  unite(document, title, chapter)

# split into words
by_chapter_word <- by_chapter %>%
  unnest_tokens(word, text)

# find document-word counts
word_counts <- by_chapter_word %>%
  anti_join(stop_words) %>%
  count(document, word, sort = TRUE) %>%
  ungroup()

word_counts
```
For at lave topic modelling skal vi nu have transformeret det til en dokumentTermMatrix vha. cast_dtm()
```{r}
chapters_dtm <- word_counts %>%
  cast_dtm(document, word, n)

chapters_dtm

```
Vi kan ny lave fire temaer, vi ved der er fire temaer, da der er 4 bøger. I andre tilfælde skal man prøve sig frem.

```{r}
chapters_lda <- LDA(chapters_dtm, k = 4, control = list(seed = 1234))
chapters_lda
```
Lad os se på topic-per-word
```{r}
chapter_topics <- tidy(chapters_lda, matrix = "beta")
chapter_topics
```
Lad os finde fem termer fra hvert tema:

```{r}

top_terms <- chapter_topics %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms
```

Lad os visualisere dette:

```{r}

library(ggplot2)

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()
```
Nu vil vi vide hvilke ord der er associeret med hvert dokument. Hvad er per-dokument-per-topic sandsynligheden (gamma)?

```{r}
chapters_gamma <- tidy(chapters_lda, matrix = "gamma")
chapters_gamma

```

Nu skal vi separerer dokumenter så vi får titel og kapitel:

```{r}

chapters_gamma <- chapters_gamma %>%
  separate(document, c("title", "chapter"), sep = "_", convert = TRUE)

chapters_gamma
```

```{r}
# reorder titles in order of topic 1, topic 2, etc before plotting
chapters_gamma %>%
  mutate(title = reorder(title, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ title)
```
Der er nogle kapitler der kommer fra forskellige bøger ved Great expectations. Kan vi finde ud af, hvad der foregår?
```{r}
chapter_classifications <- chapters_gamma %>%
  group_by(title, chapter) %>%
  top_n(1, gamma) %>%
  ungroup()

chapter_classifications
```
Lad os se, hvilke bøger, der oftest er misidentificeret:
```{r}

book_topics <- chapter_classifications %>%
  count(title, topic) %>%
  group_by(title) %>%
  top_n(1, n) %>%
  ungroup() %>%
  transmute(consensus = title, topic)

chapter_classifications %>%
  inner_join(book_topics, by = "topic") %>%
  filter(title != consensus)
```

Hvilke ord i hvert dokument er tilegnet et givent tema? Til dette kan vi bruge augment()  funktionen.

```{r}

assignments <- augment(chapters_lda, data = chapters_dtm)
assignments
```
augment fjerner eksisterende kolonner, derfor kan vi bruge join til at tilføje de origianle kolonner. 
```{r}
assignments <- assignments %>%
  separate(document, c("title", "chapter"), sep = "_", convert = TRUE) %>%
  inner_join(book_topics, by = c(".topic" = "topic"))

assignments
```

VI kan nu visualisere, hvor ofte kapitler blev tildelt de forkerte bøger:

```{r}

library(scales)

assignments %>%
  count(title, consensus, wt = count) %>%
  group_by(title) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(consensus, title, fill = percent)) +
  geom_tile() +
  scale_fill_gradient2(high = "red", label = percent_format()) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        panel.grid = element_blank()) +
  labs(x = "Book words were assigned to",
       y = "Book words came from",
       fill = "% of assignments")
```
The great expectations havde mange ord, der blev tildelt de forkerte kapitler. Lad os finde ud af hvilke ord:

```{r}
wrong_words <- assignments %>%
  filter(title != consensus)

wrong_words
```

```{r}
wrong_words %>%
  count(title, consensus, term, wt = count) %>%
  ungroup() %>%
  arrange(desc(n))
```

Lad os kigge på et af ordene:

```{r}
word_counts %>%
  filter(word == "flopson")
```

Vi kan også bruge mallet til at lave topic modelling. 
```{r}
#install.packages("mallet")
library(mallet)

# create a vector with one string per chapter
collapsed <- by_chapter_word %>%
  anti_join(stop_words, by = "word") %>%
  mutate(word = str_replace(word, "'", "")) %>%
  group_by(document) %>%
  summarize(text = paste(word, collapse = " "))

# create an empty file of "stopwords"
file.create(empty_file <- tempfile())
docs <- mallet.import(collapsed$document, collapsed$text, empty_file)

mallet_model <- MalletLDA(num.topics = 4)
mallet_model$loadDocuments(docs)
mallet_model$train(100)
```
VI kan nu omdanne dette til et tidy format:
```{r}
# word-topic pairs
tidy(mallet_model)

# document-topic pairs
tidy(mallet_model, matrix = "gamma")

# column needs to be named "term" for "augment"
term_counts <- rename(word_counts, term = word)
augment(mallet_model, term_counts)
```

##Opsummering
Topic modelling finder klynger af ord, der karakteriserer et sæt dokumenter. Tidy() lader os herefter forstå disse modeller ved at bruge dplyr og gglot2. 

#Cases korte noter
Til twitter kan man bruge tweets som en tokinser.