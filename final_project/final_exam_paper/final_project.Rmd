---
title: "final Project"
author: "Christoffer M. Kramer"
date: "26/11/2020"
output:
  html_document:
    df_print: paged
  word_document: default
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  results = "hide"
)
```

# Introduction
Ever since the 2016 presidential election, which ended with Donald Trump becoming president, American politics and the American populace have become more polarized than ever. However, Donald Trump’s victory was not a sudden event. It was the culmination of increasing political polarization in the US, which has been documented by countless studies from the Pew research center (pewresearch.org). But can this polarization also be seen in election debates? I’m going to do an exploratory analysis of the debates to answer this question. This is done with simple text-mining techniques, such as sentiment analysis, stop-word filtering, term frequency-inverse document frequency, and data-visualizations.  
This analysis is only preliminary, and I will conclude this paper by suggesting, how polarization among presidential and vice-presidential candidates can be further investigated based on the results. 

```{r load libraries, include = FALSE}
library(tidyverse) #Cleaning data
library(tidytext) #Text-mining
library(ggplot2) #Plotting
library(ggwordcloud) #Plotting word clouds
library(rvest)  #Web scraping
library(lubridate) #Cleaning Dates
library(zoo) #Dealing with na's
```


# Web scraping
I need to web-scrape each transcript since 1960. This is done in the code chunk below. For an explanation of this code see pp. X-X, FIXME, or [my github repository for that portfolio](https://github.com/Digital-Methods-HASS/au590388_Christoffer_Kramer/tree/master/learning_journal_and_assignments/week_44_webscrape). 

```{r - web scraping_vect_link}
# Scrape Debate function --------------------------------------------------
scrape_debates <- function(website) {
  p_html <-  read_html(website) %>%
    html_nodes("p") %>%
    html_text()

  vect_p_html <- c(p_html) #Save the output in a vector.
}

# Get and store links to debates ---------------------------------------------------
link_html <- read_html("https://www.debates.org/voter-education/debate-transcripts/") %>%
  html_nodes("blockquote") %>%
  html_children() %>%
  html_nodes("a")  %>%
  html_attr("href")

vect_link <- c(link_html) #save the output in a vector
```

When I wrote the portfolio on web scraping, the transcripts for the 2020 debates had not yet been uploaded. Now that they have, I need to do some additional cleaning. I will, therefore, remove the hostname ("https://www.debates.org/) from the strings and replace them with the path to each transcript:

```{r web scraping clean links}
#Clean messy links
vect_link[1] <- "/voter-education/debate-transcripts/september-29-2020-debate-transcript/"
vect_link[2] <- "/voter-education/debate-transcripts/vice-presidential-debate-at-the-university-of-utah-in-salt-lake-city-utah/"
vect_link[3] <- "/voter-education/debate-transcripts/october-22-2020-debate-transcript/"
```

I have made two changes since I wrote my last portfolio. Rather than creating a new object for each debate transcript, I start by creating an empty tibble called all_debates_raw and row bind each debate transcript. I also removed the vector debate_names, since the row-binding makes it redundant:

```{r web scraping for loop}
# Loop through the links and store the content ----------------------------
all_debates_raw <- tibble()

for (link in vect_link[!is.na(vect_link)]) {
  date <- str_extract(link, "[A-Za-z]+-\\d+-\\d+")

 if(is.na(date)) {

  date <- str_extract(link, ".+")

 } #end if

  website <- paste0("https://www.debates.org/", link)
  debate <- scrape_debates(website) #create an object storing the transcript
  debate <- tibble(line = 1:length(debate), text = debate, date = date)
  all_debates_raw <- bind_rows(all_debates_raw, debate) #row bind the transcript to all_debates 
} #end loop
```

To make my data reproducible, I will save my results from the web-scraping in a CSV-file. By doing this, I can ensure, that my data stays intact even if the website www.debates.org ceases to exist. This makes my results more reproducible since researchers can reproduce my results without web-scraping:
```{r web scraping write csv}
write.csv(all_debates_raw, "../data/all_debates_raw.csv", row.names=FALSE) #Save as csv
```

# Data Cleaning
I have created a data set called “candidates_since 1960.csv”, which contains a list of presidential and vice-presidential candidates since 1960. This data set will be used for data wrangling. Before using the data set, I’m mutating all last names to uppercase. This is done to easier match the last names in the transcripts with a regex since those are written in uppercase:
```{r create tibble candidates}
candidates <- read.csv(file = "../data/candidates_since_1960.csv", sep = ";") #Load csv
candidates <- tibble(candidates) %>%  #make data frame a tibble.
  mutate(last_name = toupper(last_name)) #Make all names uppercase
```

I then create the tibble all_debates which contains the csv file all_debates_raw.csv:
```{r create tibble transcripts}
# Save transcripts as csv and create a tibble --------------------------------------------------------------
all_debates <- read.csv("../data/all_debates_raw.csv") #load csv
all_debates <- tibble(all_debates) #make data frame a tibble
```

This data set still needs a lot of cleaning, in order to follow Hadley Wickhams principles for tidy data (Wickham, 2014). The data does not tell, who is speaking or  what their party affiliation is. Moreover, the dates do not follow the datacarpetry's recommendation of seperating year, month and day into seperate column (datacarpentry.org, n.d.). 

## Cleaning dates
I will start the data wrangling by cleaning the dates. This is done by creating a function, that uses str_replace_all to replace a pattern with a new string:
```{r cleaning clean dates function}
#Functions that cleans names by replacing an existing string with a new string
clean_dates <- function(dataset, old_pattern, new_replacement) {
  mutate_if(dataset,
            is.character,
            str_replace_all, pattern = old_pattern, replacement = new_replacement)
}
```

Now I need to find all dates, which aren’t formatted properly. I can do this by inverting a regex with the function filter from the tidyverse package (Wickham, et al., 2019) and grepl. I then store the output in an object called wrong_dates:
```{r find dates}
#Find the wrong dates
wrong_dates <- all_debates %>% 
  filter(!grepl("[A-Za-z]+-\\d+-\\d+", date)) %>%
  select(date) %>% 
  unique()
```

There 4 wrong dates. One of them is a link to the translations of the debate transcripts from 2008. I don’t want this transcript, I will, therefore, remove it:
```{r remove translation}
#Remove translations page from list of dates and the dataset
all_debates <- all_debates[!(all_debates$date == "/voter-education/debate-transcripts/2000-debate-transcripts-translations/"),]
```

The last 3 dates aren’t properly formatted. I will, therefore, replace them with the correct date manually using my function clean_dates:
```{r replace wrong dates}
#Replace wrong dates with correct dates
all_debates <- clean_dates(all_debates, old_pattern = "voter-education/debate-transcripts/vice-presidential-debate-at-the-university-of-utah-in-salt-lake-city-utah/",
                           new_replacement = "october-7-2020")

all_debates <- clean_dates(all_debates, old_pattern ="/voter-education/debate-transcripts/2008-debate-transcript/",
                           new_replacement = "september-26-2008")

all_debates <- clean_dates(all_debates, old_pattern ="/voter-education/debate-transcripts/2008-debate-transcript-2/",
                           new_replacement = "october-2-2008")
```

I still need to separate day, month, and year, into different columns. This is done with the lubridate package (Grolemund & Wickham, 2011):
```{r cleaning dates with lubridate}
# Make "date" a proper date with different columns for day, month and year ------------------------------------------------
all_debates <- all_debates %>% 
  mutate(date = mdy(date)) %>% 
  mutate(day = day(date),
         month = month(date),
         year = year(date)
  )
```

Now I’m ready to differentiate between speakers.

## Who is speakning?
In each transcript, different speakers are marked at the start of a paragraph with their last name in uppercase followed by a semicolon. Some speakers have lowercase letters in their names (e.g. the moderator “McGee”) and in older debates, speakers are referred to as “MR”. or “MS”. I will, therefore, use a series str_extract and regexes to match all speakers, and gradually remove semicolon and “MR.” or “MS.”. The output is saved in a tibble called last_name:
```{r cleaning extract names}
# Find all names and save as a tibble
last_name <- str_extract(all_debates$text, "^[A-Za-z]+:|^M[RS]\\..+:") %>% #Find name
  str_extract("[A-Za-z]+:") %>% #remove MR. and MS.
  str_extract("[A-Za-z]+") #Remove semicolon
last_name <- tibble(last_name) #Make last_name a tibble
```

I then combine all_debates and last_name:
```{r cleaning bind names}
# bind column to all debates
all_debates <- cbind(all_debates, last_name)
```

Every time a speaker is not mentioned, it is the last known speaker who is talking. By replacing all NA’s with the previously known value, it is, therefore, possible to assign a speaker to each row. This is done with the function na.locf from the zoo package (Zeileis & Grothendieck, 2005):
```{r nalocf}
all_debates <- na.locf(all_debates) #fill out every cell with the last known value.
```

I will then transform the column last_name to uppercase so I can do a left_join:
```{r toupper names}
all_debates <- all_debates %>% 
  mutate(last_name = toupper(last_name)) # Make all last_names uppercase 
```

I then left_join all_debates with candidates by column last_name and year. I’m also replacing NA’s with the string “not_a_candidate”. This is done to follow Broman and Woo’s principles of data organization in spreadsheets. One of these principles states that NA’s should not be included in spreadsheets (Broman & Woo: 4, 2018).
```{r cleaning combine tibbles}
#Left_join candidates and all_debates by last_name and year
all_debates <- left_join(all_debates, candidates, by = c("last_name" = "last_name", "year" = "year"))
all_debates[is.na(all_debates)] <- "not_a_candidate" #fill NA's
```

Since each speaker is declared in the column last_name. I’m going to use the same regex as previously for finding names, and then remove them from the transcripts:
```{r cleaning remove names}
# Remove the names from the text
 all_debates <- all_debates %>%
   mutate(text = str_remove(text, "^[A-Za-z]+:|^M[RS]\\..+:"))
```

Lastly, I will reorder the columns so first_name and last_name are placed in front of the text. This is purely an aesthetic choice.
```{r cleaning reorder columns}
# Reorder columns
all_debates <- all_debates %>% 
  relocate(last_name, .before = text)

#Reorder columns
all_debates <- all_debates %>% 
  relocate(first_name, .before = last_name)

```

Now my data set is clean and ready to be analyzed.

# Text-mining: Preparation 
I will do three types of text-mining: tf-idf, sentiment analysis, and stop word filtering. I'm mainly inspired by Julia Silge and David Robinson’s approach to text-mining in their book: "Text Mining with R: A Tidy Approach" (2020). Therefore, I'm going to use the tidytext package (Silge & Robinsong, 2016).
I start by creating a custom function for plotting word clouds. Most parameters have a default value, which will make it a lot easier to plot, but still give me room for customization:
```{r save plot, include=FALSE}
#Function for saving plots. Removed in the final portfolio
save_plot_pdf <- function(name, path_destination = "../plots/", width_value = 8.26, height_value = 11.69){
  dev.copy(pdf, paste(path_destination, name, ".pdf", sep = ""), width= width_value, height= height_value) #Units are inches
  dev.off()
}

```

```{r plotting worcloud function}
# Function for plotting word clouds
plot_wordclouds <- function(data_set, label_value = word, size_value = n, max_size = 10, color_value = n, shape_value ="diamond") {
    ggplot(data_set, aes(label = {{label_value}}, size = {{size_value}})) +
    geom_text_wordcloud_area(aes(color = {{color_value}}), shape = shape_value) +
    scale_size_area(max_size) +
    theme_minimal()    
    }
```

I then create a tibble called my_stop_words, which contains stopwords that are not included in the stop word lexicon. Afterward, I tokenize the text by word and use a regex to filter out digits. Lastly, by using anti_join, I filter out rows that match my_stop_words. The output is saved in the object debate_words: 
```{r Tokenize by word}
my_stop_words <- tibble(word = c("uh", "uhh")) #custom stop words

debate_words <- all_debates %>% 
  unnest_tokens(word, text) %>% 
  filter(!grepl("[[:digit:]]", word)) %>%  #Remove digits
  anti_join(my_stop_words)
```
Now I’m ready for text-mining.  

## Text-mining: Stop Word filtering
Most transcripts use a single right quotation mark instead of an apostrophe. This causes problems when using the anti_join since my stop word lexicon contains proper apostrophes. This is a problem caused by the font in the transcripts, which does not differentiate between right single quotation marks and apostrophes. Luckily, I found an answer to the problem in this stack overflow question, which is used on the second line. I then do an anti_join to remove stop words. The output is saved in the object stop_words_removed:
```{r stop word filtering}
stop_words_removed <- debate_words %>% 
  mutate(word = gsub("\u2019", "'", word)) %>%  #Read right single quotation mark as an apostrophe 
  anti_join(stop_words)
```
Now I’m ready to plot my results.

### Plotting Stop Word filtering
I will plot my results by filtering out rows where the candidate does not belong to the republican or democratic party, from the object stop_words_removed. Then I count by word and year, and then pipe the result directly into my function plot_wordclouds, which will be faceted by year and party:
```{r plotting democrats wordcloud stop words, fig.cap="Fig. 1: Stop Word Filtering"}
stop_words_removed %>% 
  filter(grepl("[DR]", party)) %>% 
  count(year, party, word, sort = TRUE) %>% 
  group_by(year, party) %>% 
  slice_max(order_by = n, n = 20) %>% 
  plot_wordclouds() +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
  facet_wrap(~year + party) +
  labs(title = "Fig. 1: Stop Word Filtering")
  
save_plot_pdf("fig_1")
```
As fig. 1 shows, the candidates’ primary talking points from 2008-2016 appears to be the opposite candidate. It is quite astonishing that the opponent’s name appears so often, that otherwise common themes, such as national debt, jobs, wars, or budgets, are pushed aside. However, this result might be a consequence of changes to the debate format, such as longer debates, which naturally would increase how often a candidate is mentioned.
By using term frequency-inverse document frequency, which looks at the word frequency across all documents, it should be clearer whether this is a general phenomenon or the result of changes to the debate format.


## Text-mining: Term Frequency-Inverse Document Frequency
I start the tf-idf analysis by counting how often a word in the data set debate_words appear each year and save the output in an object called word_count_year:
```{r tf_idf count}
word_count_year <- debate_words %>% 
  count(year, word, sort = TRUE)
```

Then I calculate the total amount of words each year and save the result in an object called total_words_year.
```{r tf_idf total}
total_words_year <- word_count_year %>%
  group_by(year) %>% 
  summarise(total = sum(n))
```
I then left join total_words_year and word_count_year and save the result in word_count_year.
```{r tf_idf left join}
word_count_year <- left_join(word_count_year, total_words_year)
```
Lastly, I calculate tf-idf for each word in each year, by using the function bind_tf_idf from the tidytext package and save the output in an object called word_tf_idf_year:
```{r tf_idf bind_tf_idf}
word_tf_idf_year <- word_count_year %>% 
  bind_tf_idf(word, year, n)
```

Let's plot it:
```{r TF_IDF plotting, fig.cap="Fig. 2: TF-IDF each year - Both Parties"}
word_tf_idf_year %>%
  group_by(year) %>%
  slice_max(order_by = tf_idf, n = 20) %>%
 plot_wordclouds(size_value = tf_idf, color_value = tf_idf) +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
   facet_wrap(~year, ncol = 3, scales = "free") +
  labs(title = "Fig. 2: Tf-idf each year - Both Parties")

save_plot_pdf("fig_2")
```
The stop word filtering suggests that each candidate increasingly talks about their opponent. The tf-idf analysis substantiates this point, since it shows, that each debate increasingly revolves around the candidates themselves rather than political issues. This tendency started back in 1992 and was especially pronounced in 1996 and from 2004 to 2020. These results certainly point towards increasing polarization, since I doubt that the candidates are praising their opponents. But what about their sentiment, has it changed over the last couple of years, and does it differ between the parties? This can be explored through a sentiment analysis. 

## Text-mining: Sentiment Analysis
To get reliable results, I will only focus on debates after 1992. I will start by saving the words “Trump” and “vice” in a tibble called distortion_words. This tibble is used to anti_join these words since they distort the result. Trump is, in this case, a name, but is defined as a word with a positive sentiment in most sentiment lexicons. Vice refers to vice-presidents, but it has a negative sentiment in most lexicons:
```{r sentiment analysis distortion}
distortion_words <- tibble(word = c("trump", "vice"))
```

I then filter out all debates before 1992, remove the distortion words, and inner_joins the binary sentiment lexicon “bing”. The result is saved in the object sentiment_debate_words:
```{r sentiment analysis prepare}
sentiment_debate_words <- debate_words %>% 
  filter (year >= 1992) %>%
  anti_join(distortion_words) %>% 
  inner_join(get_sentiments("bing"))
```
Now I should be ready to plot the sentiments. 

### Ploting Sentiments
I want to make a pie-chart to visualize the bing values. Many authors reject pie-charts, but, as Claus Wilke points out, they work well when showing simple fractions such as one-half, one-third, one-quarter, and so on (Wilke, XXXX - FIXME). Moreover, since I only have two values, the pie-chart is a useful tool for visualizing how large a portion each sentiment makes up.
I make a function called plot_pie_chart, which calculates how large a share each sentiment makes up of the total words and use coord_polar to make the bar chart round rather than rectangular:
```{r plot pie char function}
#Plot Pie chart function
plot_pie_chart <- function(data_set, fill_value = sentiment){
  data_set %>%  
  mutate(total = sum(n)) %>% 
  mutate(share = n/total) %>% 
  ggplot() + 
  geom_bar(aes(x = "", y = share, fill = {{fill_value}}), stat = "identity") +
  scale_fill_brewer(palette = "Set3") +
  coord_polar("y", start = 0) +
  theme(axis.text.x = element_blank()) 
  }

```

Let's start with the republicans. Because I need to summarize the result, I will use the function tally instead of count:

```{r pie chart republicans all years, fig.cap="Fig. 3: Republicans' sentiment for each year - BING"}
#Republicans
sentiment_debate_words %>%   
  filter(party == "R") %>% 
  group_by(year, sentiment) %>%
  tally() %>%
  plot_pie_chart() +
   facet_wrap(~year, ncol = 7) +
  labs(title = "Fig. 3: Republicans' sentiment by year - BING",
       x = "",
       y = "",
       fill = "sentiments")

save_plot_pdf("fig_3")
```
The year 2000 was the most positive debate for the republicans, and all subsequent years have since then become more negative, culminating in 2016, where almost half of all counted words had a negative sentiment. 
Let’s look at the democrats:

```{r pie chart democrats all years, fig.cap="Fig. 4: Democrats' sentiment for each year - BING"}
#Democrats
sentiment_debate_words %>%
  filter(party == "D") %>% 
  filter(year >= 1992) %>% 
  group_by(year, sentiment) %>% 
  tally() %>% 
  plot_pie_chart() +
  facet_wrap(~year, ncol = 7) +
  labs(title = "Fig. 4: Democrats sentiment by year - BING",
       x = "",
       y = "",
       fill = "sentiments")

save_plot_pdf("fig_4")
```
Democrats almost follow the same pattern, with the debate in 2000 having an extremely positive sentiment, and subsequent debates being much more negative culminating in 2020, where about half of the counted words were negative. The 2012 debate is also interesting since democrats have a surprisingly positive sentiment. 

# Conclusion 
This preliminary exploratory analysis already points towards some interesting tendencies. Based on the tf-idf analysis it does look like debates increasingly revolve around the individual candidates rather than policy issues. This tendency appears to have started in 1992 but was especially pronounced in 1996, 2004, 2008, 2016, and 2020.  The stop word filtering points towards the same results and suggests that candidates are primarily mentioned by their opponent rather than by themselves or their vice-presidential candidate. The sentiment analysis shows that both parties have generally developed a more negative sentiment since 2000 culminating in 2016 for the republicans and 2020 for democrats. The results, therefore, point towards an increased polarization, where each candidate spends more time attacking their opponent. This suggests that rather than looking at differences in political topics or opinions, further research into polarization among political candidates could also investigate how often and in what context each candidate addresses their opponent. 

# Literature
- Broman, W. K., & Woo, H. K. (2018). ”Data Organization in Spreadsheets”. The American Statistician, 72:1, 2-10, DOI: 10.1080/00031305.2017.1375989
- Pewreseach.org (n.d.): ”Political Polarization”. Pew Research Center. URL: https://www.pewresearch.org/topics/political-polarization/ 
- Silge J, Robinson D (2016). “Tidytext: Text Mining and Analysis Using Tidy Data Principles in R.” JOSS, 1(3). doi: 10.21105/joss.00037, http://dx.doi.org/10.21105/joss.00037
- Wilke, C. O. (2019). “Chapter 10: Visualizing proportions” in Fundamentals of Data Visualization. O’Reilly Media Inc., URL: https://clauswilke.com/dataviz/visualizing-proportions.html 

## Packages
- Fellows, I. (2018). "wordcloud: Word Clouds". Located at CRAN here: https://cran.r-project.org/web/packages/wordcloud/index.html 
- Grolemund, G. & Wickham, H. (2011). ”Dates and Times Made Easy with lubridate”. Journal of Statistical Software, 40(3), 1-25. URL http://www.jstatsoft.org/v40/i03/
- Silge, J. & Robinson, D. (2017). “Text Mining with R: A Tidy Approach”. O’reilly Media, Inc. URL: https://www.tidytextmining.com/ 
- Wickham, H (2016). “ggplot2: Elegant Graphics for Data Analysis.” Springer-Verlag New York. ISBN 978-3-319-24277-4, https://ggplot2.tidyverse.org 
- Wickham, H. (2020). “rvest: Easily Harvest (Scrape) Web Pages”. Located at CRAN here: https://cran.r-project.org/web/packages/rvest/index.html 
- Wickham H et al. (2019). “Welcome to the tidyverse.” Journal of Open Source Software, 4(43), 1686. doi: 10.21105/joss.01686
- Zeileis, A. & Grothendieck, G. (2005). “zoo: S3 Infrastructure for Regular and Irregular Time Series.” Journal of Statistical Software, 14(6), 1–27. doi: 10.18637/jss.v014.i06

